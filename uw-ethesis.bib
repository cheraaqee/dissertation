% Bibliography of key references for "LaTeX for Thesis and Large Documents"
% For use with BibTeX


@misc{del,
title = {{The Zettabyte Era: Trends and Analysis - Cisco, 2018}},
url = {https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/vni-hyperconnectivity-wp.html},
urldate = {2018-05-07}
}
@article{Ghadiyaram2016,
abstract = {Most publicly available image quality databases have been created under highly controlled conditions by introducing graded simulated distortions onto high-quality photographs. However, images captured using typical real-world mobile camera devices are usually afflicted by complex mixtures of multiple distortions, which are not necessarily well-modeled by the synthetic distortions found in existing databases. The originators of existing legacy databases usually conducted human psychometric studies to obtain statistically meaningful sets of human opinion scores on images in a stringently controlled visual environment, resulting in small data collections relative to other kinds of image analysis databases. Towards overcoming these limitations, we designed and created a new database that we call the LIVE In the Wild Image Quality Challenge Database, which contains widely diverse authentic image distortions on a large number of images captured using a representative variety of modern mobile devices. We also designed and implemented a new online crowdsourcing system, which we have used to conduct a very large-scale, multi-month image quality assessment subjective study. Our database consists of over 350000 opinion scores on 1162 images evaluated by over 7000 unique human observers. Despite the lack of control over the experimental environments of the numerous study participants, we demonstrate excellent internal consistency of the subjective dataset. We also evaluate several top-performing blind Image Quality Assessment algorithms on it and present insights on how mixtures of distortions challenge both end users as well as automatic perceptual quality prediction models.},
archivePrefix = {arXiv},
arxivId = {1511.02919},
author = {Ghadiyaram, Deepti and Bovik, Alan C.},
doi = {10.1109/TIP.2015.2500021},
eprint = {1511.02919},
file = {::},
isbn = {1057-7149 VO - 25},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Authentic distortions,CLIVE,Crowdsourcing,Ghadiyaram2016,Perceptual image quality,Subjective image quality assessment,Wild,database},
mendeley-groups = {D A T A B A S E S,2- MLIVE-cited},
mendeley-tags = {CLIVE,Ghadiyaram2016,Wild,database},
number = {1},
pages = {372--387},
pmid = {26571530},
title = {{Massive online crowdsourced study of subjective and objective picture quality}},
volume = {25},
year = {2016}
}
@inproceedings{Kang2014,
abstract = {In this work we describe a Convolutional Neural Network (CNN) to accurately predict image quality without a reference image. Taking image patches as input, the CNN works in the spatial domain without using hand-crafted features that are employed by most previous methods. The network consists of one convolutional layer with max and min pooling, two fully connected layers and an output node. Within the network structure, feature learning and regression are integrated into one optimization process, which leads to a more effective model for estimating image quality. This approach achieves state of the art performance on the LIVE dataset and shows excellent generalization ability in cross dataset experiments. Further experiments on images with local distortions demonstrate the local quality estimation ability of our CNN, which is rarely reported in previous literature.},
author = {Kang, Le and Ye, Peng and Li, Yi and Doermann, D},
booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.224},
file = {::},
isbn = {VO -},
issn = {10636919},
keywords = {CNN,Convolutional Neural Network,Feature extraction,Image Quality,Image quality,Kernel,Neural networks,No Reference,Nonlinear distortion,Training,Transform coding,cnn,convolutional neural networks,feature learning,generalisation (artificial intelligence),generalization ability,image patches,image processing,image quality assessment,image quality estimation,image quality prediction,learning (artificial intelligence),max pooling,min pooling,neural nets,no-reference image quality assessment,optimization process,regression analysis},
mendeley-groups = {Deep Image,IQA-NR},
mendeley-tags = {CNN,Image Quality,No Reference,cnn},
pages = {1733--1740},
title = {{Convolutional Neural Networks for No-Reference Image Quality Assessment}},
year = {2014}
}
@article{Bosse2018,
author = {Bosse, Sebastian and Maniry, Dominique and Muller, Klaus-Robert and Wiegand, Thomas and Samek, Wojciech},
doi = {10.1109/TIP.2017.2760518},
file = {::},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
keywords = {Full Reference,Mixed Reference,No Reference},
mendeley-groups = {Deep Image,IQA-NR,IQA-MR,IQA-FR,IQA-RR},
mendeley-tags = {Full Reference,Mixed Reference,No Reference},
month = {jan},
number = {1},
pages = {206--219},
title = {{Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment}},
url = {http://ieeexplore.ieee.org/document/8063957/},
volume = {27},
year = {2018}
}
@article{torkamani2018image,
author = {Torkamani-Azar, Farah and Parkkinen, Jussi},
journal = {Signal, Image and Video Processing},
number = {7},
pages = {1337--1344},
publisher = {Springer},
title = {{Image quality assessment using block-based weighted SVD}},
volume = {12},
year = {2018}
}
@article{Chandler2013,
abstract = {{\textless}p{\textgreater}Image quality assessment (IQA) has been a topic of intense research over the last several decades. With each year comes an increasing number of new IQA algorithms, extensions of existing IQA algorithms, and applications of IQA to other disciplines. In this article, I first provide an up-to-date review of research in IQA, and then I highlight several open challenges in this field. The first half of this article provides discuss key properties of visual perception, image quality databases, existing full-reference, no-reference, and reduced-reference IQA algorithms. Yet, despite the remarkable progress that has been made in IQA, many fundamental challenges remain largely unsolved. The second half of this article highlights some of these challenges. I specifically discuss challenges related to lack of complete perceptual models for: natural images, compound and suprathreshold distortions, and multiple distortions, and the interactive effects of these distortions on the images. I also discuss challenges related to IQA of images containing nontraditional, and I discuss challenges related to the computational efficiency. The goal of this article is not only to help practitioners and researchers keep abreast of the recent advances in IQA, but to also raise awareness of the key limitations of current IQA knowledge.{\textless}/p{\textgreater}},
archivePrefix = {arXiv},
arxivId = {905685},
author = {Chandler, Damon M.},
doi = {10.1155/2013/905685},
eprint = {905685},
isbn = {0780374029},
issn = {2090-505X},
journal = {ISRN Signal Processing},
pmid = {15376593},
title = {{Seven Challenges in Image Quality Assessment: Past, Present, and Future Research}},
year = {2013}
}
@inproceedings{lin2010perceptual,
author = {Lin, Weisi and Narwaria, Manish},
booktitle = {Visual Communications and Image Processing 2010},
file = {::;::},
keywords = {Image Quality,Survey},
mendeley-groups = {IQA-MR,Survey- Image},
mendeley-tags = {Image Quality,Survey},
organization = {International Society for Optics and Photonics},
pages = {774403},
title = {{Perceptual image quality assessment: recent progress and trends}},
volume = {7744},
year = {2010}
}
@article{xu2017no,
author = {Xu, Shaoping and Jiang, Shunliang and Min, Weidong},
file = {::},
journal = {IETE Technical Review},
keywords = {Image Quality,Prof,Survey},
mendeley-groups = {Survey- Image},
mendeley-tags = {Image Quality,Prof,Survey},
number = {3},
pages = {223--245},
publisher = {Taylor {\&} Francis},
title = {{No-reference/blind image quality assessment: a survey}},
volume = {34},
year = {2017}
}
@article{Manap2015,
abstract = {Over the last two decades, there has been a surge of interest in the research of image quality assessment due to its wide applicability to many domains. In general, the aim of image quality assessment algorithms is to evaluate the perceptual quality of an image using an objective index which should be highly consistent with the human subjective index. The objective image quality assessment algorithms can be classified into three main classes: full-reference, reduced-reference, and no-reference. While full-reference and reduced-reference algorithms require full information or partial information of the reference image respectively, no reference information is required for no-reference algorithms. Consequently, a no-reference (or blind) image quality assessment algorithm is highly preferred in cases where the availability of any reference information is implausible. In this paper, a survey of the recent no-reference image quality algorithms, specifically for non-distortion-specific cases, is provided in the first half of this paper. Two major approaches in designing the non-distortion-specific no-reference algorithms, namely natural scene statistics-based and learning-based, are studied. In the second half of this paper, their performance and limitations are discussed before current research trends addressing the limitations are presented. Finally, possible future research directions are proposed towards the end of this paper.},
author = {Manap, Redzuan Abdul and Shao, Ling},
doi = {10.1016/j.ins.2014.12.055},
issn = {00200255},
journal = {Information Sciences},
keywords = {Blind image quality assessment,Image quality assessment,Learning-based,Natural scene statistics,No-reference image quality assessment,Non-distortion-specific},
title = {{Non-distortion-specific no-reference image quality assessment: A survey}},
year = {2015}
}
@article{Chikkerur2011,
abstract = {With the increasing demand for video-based applications, the reliable prediction of video quality has increased in importance. Numerous video quality assessment methods and metrics have been proposed over the past years with varying computational complexity and accuracy. In this paper, we introduce a classification scheme for full-reference and reduced-reference media-layer objective video quality assessment methods. Our classification scheme first classifies a method according to whether natural visual characteristics or perceptual (human visual system) characteristics are considered. We further subclassify natural visual characteristics methods into methods based on natural visual statistics or natural visual features. We subclassify perceptual characteristics methods into frequency- or pixel-domain methods. According to our classification scheme, we comprehensively review and compare the media-layer objective video quality models for both standard resolution and high definition video. We find that the natural visual statistics based MultiScale-Structural SIMilarity index (MS-SSIM), the natural visual feature based Video Quality Metric (VQM), and the perceptual spatio-temporal frequency-domain based MOtion-based Video Integrity Evaluation (MOVIE) index give the best performance for the LIVE Video Quality Database.},
author = {Chikkerur, Shyamprasad and Sundaram, Vijay and Reisslein, Martin and Karam, Lina J.},
doi = {10.1109/TBC.2011.2104671},
file = {::},
isbn = {0018-9316 VO - 57},
issn = {00189316},
journal = {IEEE Transactions on Broadcasting},
keywords = {Full-reference metric,objective video quality,perceptual video quality,reduced-reference metric,survey,video},
mendeley-groups = {V I D E O},
mendeley-tags = {survey,video},
number = {2 PART 1},
pages = {165--182},
title = {{Objective video quality assessment methods: A classification, review, and performance comparison}},
url = {http://ieeexplore.ieee.org/abstract/document/5710601/},
volume = {57},
year = {2011}
}
@inproceedings{borse2014competitive,
author = {Borse, Rushikesh and Markad, Prerana},
booktitle = {Advances in Computing, Communications and Informatics (ICACCI, 2014 International Conference on},
file = {::},
keywords = {Image Quality,survey},
mendeley-groups = {Survey- Image},
mendeley-tags = {Image Quality,survey},
organization = {IEEE},
pages = {1440--1444},
title = {{Competitive analysis of existing image quality assessment methods}},
year = {2014}
}
@misc{Sheikh,
author = {Sheikh, H R and Wang, Z. and Cormack, L. and Bovik, A.C.},
keywords = {database},
mendeley-groups = {D A T A B A S E S},
mendeley-tags = {database},
title = {{LIVE Image Quality Assessment Database Release 2}}
}
@article{Chandler2010,
abstract = {The mainstream approach to image quality assessment has centered around accurately modeling the single most relevant strategy employed by the human visual system (HVS) when judging image quality (e.g., detecting visible differences, and extracting image structure/information). In this work, we suggest that a single strategy may not be sufficient; rather, we advocate that the HVS uses multiple strategies to determine image quality. For images containing near-threshold distortions, the image is most apparent, and thus the HVS attempts to look past the image and look for the distortions (a detection-based strategy). For images containing clearly visible distortions, the distortions are most apparent, and thus the HVS attempts to look past the distortion and look for the image's subject matter (an appearance-based strategy). Here, we present a quality assessment method [most apparent distortion (MAD)], which attempts to explicitly model these two separate strategies. Local luminance and contrast masking are used to estimate detection-based perceived distortion in high-quality images, whereas changes in the local statistics of spatial-frequency components are used to estimate appearance-based perceived distortion in low-quality images. We show that a combination of these two measures can perform well in predicting subjective ratings of image quality.},
author = {Chandler, Damon M.},
doi = {10.1117/1.3267105},
isbn = {1017-9909},
issn = {1017-9909},
journal = {Journal of Electronic Imaging},
keywords = {Full Reference,MAD,database},
mendeley-groups = {D A T A B A S E S,IQA-FR},
mendeley-tags = {Full Reference,MAD,database},
number = {1},
pages = {011006},
title = {{Most apparent distortion: full-reference image quality assessment and the role of strategy}},
url = {http://electronicimaging.spiedigitallibrary.org/article.aspx?doi=10.1117/1.3267105},
volume = {19},
year = {2010}
}
@article{Ponomarenko2015,
abstract = {This paper describes a recently created image database, TID2013, intended for evaluation of full-reference visual quality assessment metrics. With respect to TID2008, the new database contains a larger number (3000) of test images obtained from 25 reference images, 24 types of distortions for each reference image, and 5 levels for each type of distortion. Motivations for introducing 7 new types of distortions and one additional level of distortions are given; examples of distorted images are presented. Mean opinion scores (MOS) for the new database have been collected by performing 985 subjective experiments with volunteers (observers) from five countries (Finland, France, Italy, Ukraine, and USA). The availability of MOS allows the use of the designed database as a fundamental tool for assessing the effectiveness of visual quality. Furthermore, existing visual quality metrics have been tested with the proposed database and the collected results have been analyzed using rank order correlation coefficients between MOS and considered metrics. These correlation indices have been obtained both considering the full set of distorted images and specific image subsets, for highlighting advantages and drawbacks of existing, state of the art, quality metrics. Approaches to thorough performance analysis for a given metric are presented to detect practical situations or distortion types for which this metric is not adequate enough to human perception. The created image database and the collected MOS values are freely available for downloading and utilization for scientific purposes.},
author = {Ponomarenko, Nikolay and Jin, Lina and Ieremeiev, Oleg and Lukin, Vladimir and Egiazarian, Karen and Astola, Jaakko and Vozel, Benoit and Chehdi, Kacem and Carli, Marco and Battisti, Federica and {Jay Kuo}, C. C.},
doi = {10.1016/j.image.2014.10.009},
isbn = {0923-5965},
issn = {09235965},
journal = {Signal Processing: Image Communication},
keywords = {Image denoising,Image lossy compression,Image visual quality metrics,database},
mendeley-tags = {database},
pages = {57--77},
title = {{Image database TID2013: Peculiarities, results and perspectives}},
volume = {30},
year = {2015}
}
@article{Ponomarenko2009,
abstract = {In this paper, a new image database, TID2008, for evaluation of full-reference visual quality assessment metrics is described. It contains 1700 test images (25 reference images, 17 types of distortions for each reference image, 4 different levels of each type of distortion). Mean Opinion Scores (MOS) for this database have been obtained as a result of more than 800 experiments. During these tests, observers from three countries (Finland, Italy, and Ukraine) have carried out about 256000 individual human quality judgments. The obtained MOS can be used for effective testing of different visual quality metrics as well as for the design of new metrics. Using the designed image database, we have tested several known quality metrics. The designed test image database is freely available for downloading and utilization in scientific investigations.},
author = {Ponomarenko, Nikolay and Lukin, Vladimir and Zelensky, Alexander and Egiazarian, Karen and Astola, Jaakko and Carli, Marco and Battisti, Federica},
file = {::},
journal = {Advances of Modern {\ldots}},
keywords = {HVS,Visual quality metrics,database,denoising,image,test image databases},
mendeley-groups = {D A T A B A S E S},
mendeley-tags = {database},
number = {January 2016},
pages = {30--45},
title = {{TID2008-A database for evaluation of full-reference visual quality assessment metrics}},
url = {http://www.ponomarenko.info/tid2008.htm{\%}5Cnhttp://www.researchgate.net/publication/221678002{\_}TID2008{\_}-{\_}A{\_}Database{\_}for{\_}Evaluation{\_}of{\_}Full-Reference{\_}Visual{\_}Quality{\_}Assessment{\_}Metrics/file/3deec519dc322ee3a7.pdf},
volume = {10},
year = {2009}
}
@misc{Horita,
author = {Horita, Y and Shibata, K and Kawayoke, Y and Sazzad, Z},
title = {{MICT image quality assessment database}}
}
@inproceedings{Jayaraman2012,
abstract = {Subjective studies have been conducted in the past to obtain human judgments of visual quality on distorted images in order, among other things, to benchmark objective image quality assessment (IQA) algorithms. Existing subjective studies primarily have records of human ratings on images that were corrupted by only one of many possible distortions. However, the majority of images that are available for consumption are corrupted by multiple distortions. Towards broadening the corpora of records of human responses to visual distortions, we recently conducted a study on two types of multiply distorted images to obtain human judgments of the visual quality of such images. Further, we compared the performance of several existing objective image quality measures on the new database and analyze the effects of multiple distortions on commonly used quality-determinant features and on human ratings.},
author = {Jayaraman, Dinesh and Mittal, Anish and Moorthy, Anush K. and Bovik, Alan C.},
booktitle = {Conference Record - Asilomar Conference on Signals, Systems and Computers},
doi = {10.1109/ACSSC.2012.6489321},
file = {::},
isbn = {9781467350518},
issn = {10586393},
keywords = {Image quality assessment,Jayaraman2012,MLIVE,Multiple Distortions,Multiple distortions,Subjective study,database},
mendeley-groups = {MD,D A T A B A S E S},
mendeley-tags = {Jayaraman2012,MLIVE,Multiple Distortions,database},
pages = {1693--1697},
title = {{Objective quality assessment of multiply distorted images}},
year = {2012}
}
@article{Li2016,
abstract = {In practice, images available to consumers usually undergo several stages of processing including acquisition, compression, transmission, and presentation, and each stage may introduce certain type of distortion. It is common that images are simultaneously distorted by multiple types of distortions. Most existing objective image quality assessment (IQA) methods have been designed to estimate perceived quality of images corrupted by a single image processing stage. In this letter, we propose a no-reference (NR) IQA method to predict the visual quality of multiply-distorted images based on structural degradation. In the proposed method, a novel structural feature is extracted as the gradient-weighted histogram of local binary pattern (LBP) calculated on the gradient map (GWH-GLBP), which is effective to describe the complex degradation pattern introduced by multiple distortions. Extensive experiments conducted on two public multiply-distorted image databases have demonstrated that the proposed GWH-GLBP metric compares favorably with existing full-reference and NR IQA methods in terms of high accordance with human subjective ratings.},
author = {Li, Qiaohong and Lin, Weisi and Fang, Yuming},
doi = {10.1109/LSP.2016.2537321},
file = {::;::},
isbn = {1070-9908 VO - 23},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Feature-SVR,Image quality assessment (IQA),Multiple Distortions,No Reference,human visual system (HVS),local binary pattern (LBP),multiple distortions,no-reference (NR),structural distortion},
mendeley-groups = {IQA-NR,MD},
mendeley-tags = {Feature-SVR,Multiple Distortions,No Reference},
number = {4},
pages = {541--545},
title = {{No-reference quality assessment for multiply-distorted images in gradient domain}},
volume = {23},
year = {2016}
}
@article{Sun2017,
abstract = {In this paper, we present a new database, the multiply distorted image database (MDID), to evaluate image quality assessment (IQA) metrics on multiply distorted images. The database contains 20 reference images and 1600 distorted images. The latter images are obtained by contamination of the former with multiple distortions of random types and levels, so multiple types of distortions appear in each distorted image. Pair comparison sorting (PCS) is used as a new subjective rating method to evaluate image quality. This method allows subjects to make equal decisions on images whose difference in quality cannot be easily evaluated visually. A total of 192 subjects participated in the subjective rating, in which mean opinion scores and standard deviations were obtained. In IQA research, subjective scores and algorithm predictions are generally related by a nonlinear regression. We further propose a method to initialize the parameters of the nonlinear regression. The experiments of IQA metrics conducted on MDID validate that this database is advisable and challenging.},
author = {Sun, Wen and Zhou, Fei and Liao, Qingmin},
doi = {10.1016/j.patcog.2016.07.033},
file = {::},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Image database,Image quality assessment,Multiple Distortions,Multiply distorted images,Nonlinear regression,Pair comparison sorting,Sun2017,database},
mendeley-groups = {D A T A B A S E S,2- MLIVE-cited},
mendeley-tags = {Multiple Distortions,Sun2017,database},
pages = {153--168},
title = {{MDID: A multiply distorted image database for image quality assessment}},
volume = {61},
year = {2017}
}
@article{Gu2014,
abstract = {In a typical image communication system, the visual signal presented to$\backslash$nthe end users may undergo the steps of acquisition, compression and$\backslash$ntransmission which cause the artifacts of blurring, quantization and$\backslash$nnoise. However, the researches of image quality assessment (IQA) with$\backslash$nmultiple distortion types are very limited. In this paper, we first$\backslash$nintroduce a new multiply distorted image database (MDID2013), which is$\backslash$ncomposed of 324 images that are simultaneously corrupted by blurring,$\backslash$nJPEG compression and noise injection. We then propose a new six-step$\backslash$nblind metric (SISBLIM) for quality assessment of both singly and$\backslash$nmultiply distorted images. Inspired by the early human visual model and$\backslash$nrecently revealed free energy based brain theory, our method works to$\backslash$nsystematically combine the single quality prediction of each emerging$\backslash$ndistortion type and joint effects of different distortion sources.$\backslash$nComparative studies of the proposed SISBLIM with popular full-reference$\backslash$nIQA approaches and start-of-the-art no-reference IQA metrics are$\backslash$nconducted on five singly distorted image databases (LIVE, TID2008, CSIQ,$\backslash$nIVC, Toyama) and two newly released multiply distorted image databases$\backslash$n(LIVEMD, MDID2013). Experimental results confirm the effectiveness of$\backslash$nour blind technique. MATLAB codes of the proposed SISBLIM algorithm and$\backslash$nMDID2013 database will be available online at http://gvsp.sjtu.edu.cn/.},
author = {Gu, Ke and Zhai, Guangtao and Yang, Xiaokang and Zhang, Wenjun},
doi = {10.1109/TBC.2014.2344471},
file = {::;::},
issn = {00189316},
journal = {IEEE Transactions on Broadcasting},
keywords = {Image quality assessment (IQA),MDID2013,Multiple Distortions,SISBLIM,blind/no-reference (NR),database,free energy,human visual system (HVS),joint effects,multiply distortion types},
mendeley-groups = {MD,D A T A B A S E S,2- MLIVE-cited},
mendeley-tags = {MDID2013,Multiple Distortions,SISBLIM,database},
number = {3},
pages = {555--567},
title = {{Hybrid no-reference quality metric for singly and multiply distorted images}},
volume = {60},
year = {2014}
}
@incollection{Chetouani2016,
author = {Chetouani, Aladine},
doi = {10.1007/978-3-319-48680-2_42},
file = {:C$\backslash$:/Users/PersianNB/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chetouani - 2016 - An Image Quality Metric with Reference for Multiply Distorted Image.pdf:pdf},
keywords = {Chetouani2016,Multiple Distortions},
mendeley-groups = {MD,2- MLIVE-cited},
mendeley-tags = {Chetouani2016,Multiple Distortions},
month = {oct},
pages = {477--485},
publisher = {Springer, Cham},
title = {{An Image Quality Metric with Reference for Multiply Distorted Image}},
url = {http://link.springer.com/10.1007/978-3-319-48680-2{\_}42},
year = {2016}
}
@inproceedings{Chetouani2015,
author = {Chetouani, Aladine},
booktitle = {2015 IEEE/ACS 12th International Conference of Computer Systems and Applications (AICCSA)},
doi = {10.1109/AICCSA.2015.7507114},
isbn = {978-1-5090-0478-2},
mendeley-groups = {MD,2- MLIVE-cited},
month = {nov},
pages = {1--4},
publisher = {IEEE},
title = {{A Reduced Reference Image Quality assessment for Multiply Distorted Images}},
url = {http://ieeexplore.ieee.org/document/7507114/},
year = {2015}
}
@article{Li2018a,
abstract = {Most of real-world image distortions are multiply distortion rather than single distortion. To address this issue, in this paper we propose a quaternion wavelet transform (QWT) based full reference image quality assessment (FR IQA) metric for multiply distorted images, which jointly considers the local similarity of phase and magnitude of each subband via QWT. Firstly, the reference images and distorted images are decomposed by QWT, and then the similarity of amplitude and phase are calculated on each subband, thirdly the IQA metric is constructed by the weighting method considering human visual system (HVS) characteristics, and lastly the scores of each subband are averaged to get the quality score of test image. Experimental results show that the proposed method outperforms the state of art in multiply distorted IQA.},
author = {Li, Chaofeng and Li, Yifan and Yuan, Yunhao and Wu, Xiaojun and Sang, Qingbing},
doi = {10.1371/journal.pone.0199430},
editor = {Wang, Yuanquan},
file = {::},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {Li2018{\_}Chaofeng,Multiple Distortions,QWT-IQA},
mendeley-groups = {2- MLIVE-cited},
mendeley-tags = {Li2018{\_}Chaofeng,Multiple Distortions,QWT-IQA},
month = {jun},
number = {6},
pages = {e0199430},
publisher = {Public Library of Science},
title = {{Quaternion wavelet transform based full reference image quality assessment for multiply distorted images}},
url = {http://dx.plos.org/10.1371/journal.pone.0199430},
volume = {13},
year = {2018}
}
@article{Mahmoudpour2018,
abstract = {The majority of existing objective Image Quality Assessment (IQA) methods are designed for evaluation of images corrupted by single distortion types. However, images may be degraded with multiple distortions during processing stages. In this paper, we propose a reduced-reference IQA algorithm to predict the quality of multiply-distorted images. An image is first decomposed into predicted and disorderly portions based on the internal generative mechanism theory. The structural information is captured from the predicted image by using a shearlet representation and R{\'{e}}nyi directional entropy is deployed to measure the disorderly information changes. Finally, we introduce the application of a framework namely Learning Using Privileged Information (LUPI) to build a quality model and obtain quality scores. During training, the LUPI framework utilizes a set of additional privileged data to learn an improved quality model. Experimental results on multiply-distorted image datasets (MLIVE and MDID2015) confirm the effectiveness of the proposed IQA model.},
author = {Mahmoudpour, Saeed and Schelkens, Peter},
doi = {10.1016/j.jvcir.2018.10.027},
issn = {10959076},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Entropy analysis,Image quality,Multiply-distortion types,Privileged information,Shearlet transform,Support vector regression},
title = {{Reduced-reference quality assessment of multiply-distorted images based on structural and uncertainty information degradation}},
year = {2018}
}
@inproceedings{Mahmoudpour2017,
author = {Mahmoudpour, Saeed and Schelkens, Peter},
booktitle = {2017 Ninth International Conference on Quality of Multimedia Experience (QoMEX)},
doi = {10.1109/QoMEX.2017.7965628},
file = {::},
isbn = {978-1-5386-4024-1},
keywords = {Mahmoudpour2017,Multiple Distortions},
mendeley-groups = {2- MLIVE-cited},
mendeley-tags = {Mahmoudpour2017,Multiple Distortions},
month = {may},
pages = {1--5},
publisher = {IEEE},
title = {{Reduced-reference image quality assessment based on internal generative mechanism utilizing shearlets and R{\'{e}}nyi entropy analysis}},
url = {http://ieeexplore.ieee.org/document/7965628/},
year = {2017}
}
@inproceedings{Gu2013,
author = {Gu, Ke and Zhai, Guangtao and Liu, Min and Yang, Xiaokang and Zhang, Wenjun and Sun, Xianghui and Chen, Wanhong and Zuo, Ying},
booktitle = {SiPS 2013 Proceedings},
doi = {10.1109/SiPS.2013.6674512},
file = {::;::},
isbn = {978-1-4673-6238-2},
keywords = {FISBLIM,Gu2013,Multiple Distortions,No Reference},
mendeley-groups = {MD,2- MLIVE-cited},
mendeley-tags = {FISBLIM,Gu2013,Multiple Distortions,No Reference},
month = {oct},
pages = {241--246},
publisher = {IEEE},
title = {{FISBLIM: A FIve-Step BLInd Metric for quality assessment of multiply distorted images}},
url = {http://ieeexplore.ieee.org/document/6674512/},
year = {2013}
}
@article{Dai2018,
abstract = {Multiply-distorted images, that is, distorted by different types of distortions simultaneously, are so common in real applications. This kind of images contain multiple overlaying stages (e.g., acquisition, compression and transmission stage). Each stage will introduce a certain type of distortion, for example, sensor noise in acquisition stage and compression artifacts in compression stage. However, most current blind/no-reference image quality assessment (NR-IQA) methods are specifically designed for singly-distorted images, thus resulting in their deficiency in handling multiply-distorted images. Motivated by the hypothesis that human visual system (HVS) is adapted to the structural information in images, we attempt to assess multiply-distorted images based on structural degradation. To this end, we use both first- and high-order image structures to design a novel referenceless quality metric for multiply-distorted images. Specifically, we leverage the quality-aware features extracted from both the gradient-magnitude map and contrast-normalized map, and further improve the performance by making use of redundancy of features with random subspace method. Experimental results on popular multiply-distorted image databases verify the outstanding performance of the proposed method.},
author = {Dai, Tao and Gu, Ke and Niu, Li and bing Zhang, Yong and Lu, Weizhi and Xia, Shu Tao},
doi = {10.1016/j.neucom.2018.02.050},
file = {::},
isbn = {9781509021758},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Dai2018,Image quality assessment,Local binary pattern,Multiple Distortions,Multiple distortions,No-reference,Structural degradation},
mendeley-groups = {MD,2- MLIVE-cited},
mendeley-tags = {Dai2018,Multiple Distortions},
pages = {185--195},
title = {{Referenceless quality metric of multiply-distorted images based on structural degradation}},
volume = {290},
year = {2018}
}
@article{Hadizadeh2016,
author = {Hadizadeh, Hadi and Bajic, Ivan V.},
doi = {10.1109/LSP.2016.2617743},
file = {::},
issn = {1070-9908},
journal = {IEEE Signal Processing Letters},
keywords = {Hadizadeh2016,Jet-LBP,Multiple Distortions},
mendeley-groups = {MD,2- MLIVE-cited},
mendeley-tags = {Hadizadeh2016,Jet-LBP,Multiple Distortions},
month = {dec},
number = {12},
pages = {1717--1721},
title = {{Color Gaussian Jet Features For No-Reference Quality Assessment of Multiply-Distorted Images}},
url = {http://ieeexplore.ieee.org/document/7590106/},
volume = {23},
year = {2016}
}
@article{Yue2018,
abstract = {Perceptual image quality assessment (IQA) plays an important role in numerous applications, including image restoration, compression, enhancement, and others. Although many works have been conducted on individually distorted IQA problems and have achieved encouraging results, few studies have been conducted on multiple distorted (MD) IQA problems. Thus, limited progress has been made. In this paper, we propose a novel no reference image quality assessment (NR-IQA) method, named improved multiscale local binary pattern (IMLBP), for addressing multiply distorted IQA problems. The image structures are sensitive to image distortions, which motivates us to utilize the structural characteristics for overall image quality prediction. We improved the local binary pattern (LBP) by considering the human visual mechanism to better extract the structural information. The IMLBP contains two parts, the LBP and the radius difference LBP (DLBP). The DLBP reflects the values' changes in the radial direction. Specifically, when the radius value is small, the proposed descriptor is computed to represent microstructural information. Conversely, it represents macrostructural information when the radius becomes large. Moreover, to better mimick the human visual mechanism, the IMLBP is computed with the multiscale strategy and the operation is based on a patch unit whose size is proportional to the radius value. The frequency histogram of feature maps is transformed to feature vectors. Subsequently, a predictable function trained by the support vector regression is used to infer the overall quality score. Experimental results show that the proposed method outperforms most state-of-the-art IQA metrics on publicly available multiply distorted image databases. {\textcopyright} 1999-2012 IEEE.},
author = {Yue, Guanghui and Hou, Chunping and Gu, Ke and Ling, Nam and Li, Beichen},
doi = {10.1109/TMM.2018.2807589},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Image quality assessment (IQA),local binary pattern (LBP),multiple distortions,no reference (NR)},
title = {{Analysis of Structural Characteristics for Quality Assessment of Multiply Distorted Images}},
year = {2018}
}
@article{Zhou2018,
abstract = {Job crafting captures the active changes employees make to their own job designs in ways that can bring about numerous positive outcomes, including engagement, job satisfaction, resilience, and thriving. This briefing introduces the core ideas of job crafting theory for management students by defining it, describing why it is important, summarizing key research findings, and exploring what it means for employees, managers, and organizations.},
author = {Zhou, Yu and Li, Leida and Wu, Jinjian and Gu, Ke and Dong, Weisheng and Shi, Guangming},
doi = {10.1109/TMM.2018.2829607},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Quality evaluation,biorder structures,multiply distorted images,nonlocal statistics,spatial contrast,spatial distribution},
title = {{Blind quality index for multiply distorted images using biorder structure degradation and nonlocal statistics}},
year = {2018}
}
@article{Zhou2019,
abstract = {Perceptual quality evaluation of multiply distorted images has become a very challenging research topic. In this paper, we present a novel and efficient deep blind quality evaluator for multiply distorted images based on monogenic binary coding (MBC). Local complementary structural information and a deep learning method are employed to blindly evaluate the quality of multiply distorted images. First, a monogenic signal representation is utilized to decompose a multiply distorted image into three complementary components: orientation, phase, and magnitude. The quality-predictive features are then determined from the complementary components. Finally, the features are mapped to the human quality score of the multiply distorted image based on the deep neural network. The results on two newly established multiply distorted image subjective databases confirm that our metric has a better prediction performance than existing state-of-the-art full-reference and classical blind metrics.},
author = {Zhou, Wujie and Yu, Lu and Qian, Yaguan and Qiu, Weiwei and Zhou, Yang and Luo, Ting},
doi = {10.1016/j.jvcir.2019.03.001},
issn = {10959076},
journal = {Journal of Visual Communication and Image Representation},
keywords = {Blind prediction,Deep neural network,Local structural information,Monogenic binary coding,Quality assessment},
title = {{Deep blind quality evaluator for multiply distorted images based on monogenic binary coding}},
year = {2019}
}
@article{Fu2016,
abstract = {The past decade has witnessed a growing development of Image Quality Assessment (IQA) techniques. However, the researches of IQA with multiple distortion types are still limited especially on blind image quality assessment methods. In this paper, a Convolutional Neural Network (CNN) based method is proposed to predict the quality of multiply distorted images without references. Inspired by the early human visual model, the proposed CNN based method combines feature learning and regression for estimating the quality of multiply distorted images. The proposed network consists of one convolutional layer, one pooling layer with max and average pooling, two full connection layers and one softmax classification layer. With this network structure, the relationship between the accuracy of CNN and the prediction monotonicity of IQA is explored. Experimental results on the newly released LIVE multiply distorted image quality database verify the effectiveness of the proposed CNN based method.},
author = {Fu, J and Wang, H and Zuo, L},
doi = {10.1109/ICASSP.2016.7471841},
file = {::;::},
isbn = {VO -},
issn = {15206149},
journal = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
keywords = {Blind image quality assessment,CNN based method,Databases,Distortion,Fu2016,IQA techniques,Image Quality,Image coding,Image quality,Measurement,Multiple Distortions,Neural networks,No Reference,Training,accuracy,blind image quality assessment,cnn,convo-lutional neural network,convolutional neural networks,distorted images,feature learning,feature regression,image processing,multiply distorted image,neural nets,prediction monotonicity,regression analysis},
mendeley-groups = {IQA-NR,MD,2- MLIVE-cited},
mendeley-tags = {Fu2016,Image Quality,Multiple Distortions,No Reference,cnn},
pages = {1075--1079},
title = {{Blind image quality assessment for multiply distorted images via convolutional neural networks}},
year = {2016}
}
@article{Zhang2018,
author = {Zhang, Yi and Chandler, Damon M. and Member, Senior},
doi = {10.1109/TIP.2018.2857413},
issn = {1057-7149},
journal = {IEEE Transactions on Image Processing},
month = {nov},
number = {c},
pages = {1},
publisher = {IEEE},
title = {{Opinion-Unaware Blind Quality Assessment of Multiply and Singly Distorted Images via Distortion Parameter Estimation}},
url = {https://ieeexplore.ieee.org/document/8412573/},
volume = {PP},
year = {2018}
}
@article{lu2015no,
author = {Lu, Yanan and Xie, Fengying and Liu, Tongliang and Jiang, Zhiguo and Tao, Dacheng},
file = {::},
journal = {IEEE Signal Processing Letters},
keywords = {BoWSF,Lu2015{\_}Yanan,Multiple Distortions},
mendeley-groups = {MD,2- MLIVE-cited},
mendeley-tags = {BoWSF,Lu2015{\_}Yanan,Multiple Distortions},
number = {10},
pages = {1811--1815},
publisher = {IEEE},
title = {{No reference quality assessment for multiply-distorted images based on an improved bag-of-words model}},
volume = {22},
year = {2015}
}
@article{zhang2019full,
author = {Zhang, Yin and Bai, Xuehan and Yan, Junhua and Xiao, Yongqi and Zhang, Wanyi and Chatwin, C R and Young, R C D},
journal = {Journal of Imaging Science and Technology},
publisher = {Society for Imaging Science and Technology},
title = {{A full-reference image quality assessment for multiply distorted image based on visual mutual information}},
year = {2019}
}
@inproceedings{wang2019blind,
author = {Wang, Zhongling and Athar, Shahrukh and Wang, Zhou},
booktitle = {International Conference on Image Analysis and Recognition},
organization = {Springer},
pages = {89--101},
title = {{Blind Quality Assessment of Multiply Distorted Images Using Deep Neural Networks}},
year = {2019}
}
@article{miao2019quality,
  title={Quality assessment of images with multiple distortions based on phase congruency and gradient magnitude},
  author={Miao, Xikui and Chu, Hairong and Liu, Hui and Yang, Yao and Li, Xiaolong},
  journal={Signal Processing: Image Communication},
  year={2019},
  publisher={Elsevier}
}
@inproceedings{Alakarhu2007,
abstract = {This paper considers image sensors and image quality in camera phones. A method to estimate the image quality and performance of an image sensor using its key parameters is presented. Subjective image quality and mapping camera technical parameters to its subjective image quality are discussed. The developed performance metrics are used to optimize sensor performance for best possible image quality in camera phones. Finally, the future technology and technology trends are discussed. The main development trend for images sensors is gradually changing from pixel size reduction to performance improvement within the same pixel size. About 30{\%} performance improvement is observed between generations if the pixel size is kept the same. Image sensor is also the key component to offer new features to the user in the future},
author = {Alakarhu, Juha},
booktitle = {Proc. 2007 International Image Sensor Workshop},
title = {{Image sensors and image quality in mobile phones}},
year = {2007}
}
@article{Zhai2008,
abstract = {Most studies in the literature for video quality assessment have been focused on the evaluation of quantized video sequences at fixed and high spatial and temporal resolutions. Only limited work has been reported for assessing video quality under different spatial and temporal resolutions. In this paper, we consider a wider scope of video quality assessment in the sense of considering multiple dimensions. In particular, we address the problem of evaluating perceptual visual quality of low bit-rate videos under different settings and requirements. Extensive subjective view tests for assessing the perceptual quality of low bit-rate videos have been conducted, which cover 150 test scenarios and include five distinctive dimensions: encoder type, video content, bit rate, frame size, and frame rate. Based on the obtained subjective testing results, we perform thorough statistical analysis to study the influence of different dimensions on the perceptual quality and some interesting observations are pointed out. We believe such a study brings new knowledge into the topic of cross-dimensional video quality assessment and it has immediate applications in perceptual video adaptation for scalable video over mobile networks. {\textcopyright} 2008 IEEE.},
author = {Zhai, Guangtao and Cai, Jianfei and Lin, Weisi and Yang, Xiaokang and Zhang, Wenjun and Etoh, Minora},
doi = {10.1109/TMM.2008.2004910},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Perceptual visual quality,Subjective view test,Video adaptation,Video quality assessment},
title = {{Cross-dimensional perceptual quality assessment for low bit-rate videos}},
year = {2008}
}
@article{Zhang2011,
abstract = {Perceptual image coding requires an effective image quality metric, yet most of the existing metrics are complex and can hardly guide the compression effectively. This paper proposes a practical full-reference metric with consideration of the texture masking effect and contrast sensitivity function. The metric is capable of evaluating typical image impairments in real-world applications and can achieve the comparable performance as the state-of-the-art metrics on the publicly available subjectively-rated image databases. Due to its simplicity, the metric is embedded into JPEG image coding to ensure a better perceptual rate-distortion performance.},
author = {Zhang, Fan and Ma, Lin and Li, Songnan and Ngan, King Ngi},
doi = {10.1109/TMM.2011.2134079},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Image quality metric,perceptual coding},
title = {{Practical image quality metric applied to image coding}},
year = {2011}
}
@article{Anegekuh2015,
abstract = {{\textcopyright} 1999-2012 IEEE.A new reference-free, objective, video quality prediction model that takes into account video content type to predict the quality of streamed high efficiency video coding (HEVC) encoded video sequences is proposed. Research has shown that for the same encoder settings and network quality of service (NQoS), the video quality differs for different types of video content. This indicates that, in addition to encoder settings and NQoS, there may be other key parameters that impact video quality. In this work, we hypothesized that video content type is one of the key parameters that may impact the quality of streamed videos. Based on this assertion, temporal information is extracted from the motion vector (MV) information inherent in the encoded video bitstreams and spatial information is extracted from the quantisation parameter (QP) and the number of bits (Bits) of coded intra (I) and predictive (P) frames to develop a metric that quantifies the content type of different video sequences. The content type metric is subsequently used together with encoding QP setting and network packet loss rate (PLR) to develop a reference -free objective video quality prediction model for streamed HEVC encoded video sequences. This model has an accuracy of 92{\%} when the model predicted values of sequences not used in model derivation are compared with mean opinion score (MOS) obtained through subjective method.},
author = {Anegekuh, Louis and Sun, Lingfen and Jammeh, Emmanuel and Mkwawa, Is Haka and Ifeachor, Emmanuel},
doi = {10.1109/TMM.2015.2444098},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {Content type,crowdsourcing,high efficiency video coding (HEVC),mean opinion score (MOS),motion activity,motion estimation,motion vector (MV),picture complexity,quality of experience (QoE)},
title = {{Content-Based Video Quality Prediction for HEVC Encoded Videos Streamed Over Packet Networks}},
year = {2015}
}
@inproceedings{Gaur2014,
author = {Gaur, Aarushi and Mikolajczyk, Krystian},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2014.587},
isbn = {9781479952083},
issn = {10514651},
title = {{Ranking images based on aesthetic qualities}},
year = {2014}
}
@misc{VQEG2000,
abstract = {FRTV Phase I is the first validation experiment conducted by VQEG. This test examined Full References (FR) and No Reference (NR) objective video quality models that predicted the quality of standard definition television (625-line and 525-line). Models were submitted in 1999 and VQEG's Final Report was approved June, 2000. All NR models were withdrawn. The FRTV Phase I subjective data are included in the final report; and all video sequences are available to researchers. Models that are trained on these datasets must not be compared to the models submitted to VQEG for independent validation in 1999. Such a comparison is misleading, because the experiments contain mainly source scenes and HRCs that were unknown to the model developers},
author = {VQEG},
booktitle = {http://www.its.bldrdoc.gov/vqeg/projects/frtv phaseI},
title = {{Final report from the video quality experts group on the validation of objective quality metrics for video quality assessment}},
year = {2000}
}
@article{Wang2004,
abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a Structural Similarity Index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Wang, Zhou and Bovik, Alan Conrad and Sheikh, Hamid Rahim and Simoncelli, Eero P.},
doi = {10.1109/TIP.2003.819861},
eprint = {9411012},
file = {::},
isbn = {9781439829356},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Error sensitivity,Full Reference,Human visual system (HVS),Image Quality,Image coding,Image quality assessment,JPEG,JPEG2000,Perceptual quality,SSIM,Structural information,Structural similarity (SSIM)},
mendeley-groups = {IQA-FR},
mendeley-tags = {Full Reference,Image Quality,SSIM},
number = {4},
pages = {600--612},
pmid = {15376593},
primaryClass = {chao-dyn},
title = {{Image quality assessment: From error visibility to structural similarity}},
volume = {13},
year = {2004}
}
@article{Wang2009,
abstract = {In this article, we have reviewed the reasons why we (collectively) want to love or leave the venerable (but perhaps hoary) MSE. We have also reviewed emerging alternative signal fidelity measures and discussed their potential application to a wide variety of problems. The message we are trying to send here is not that one should abandon use of the MSE nor to blindly switch to any other particular signal fidelity measure. Rather, we hope to make the point that there are powerful, easy-to-use, and easy-to-understand alternatives that might be deployed depending on the application environment and needs. While we expect (and indeed, hope) that the MSE will continue to be widely used as a signal fidelity measure, it is our greater desire to see more advanced signal fidelity measures being used, especially in applications where perceptual criteria might be relevant. Ideally, the performance of a new signal processing algorithm might be compared to other algorithms using several fidelity criteria. Lastly, we hope that we have given further motivation to the community to consider recent advanced signal fidelity measures as design criteria for optimizing signal processing algorithms and systems. It is in this direction that we believe that the greatest benefit eventually lies.},
author = {Wang, Zhou and Bovik, Alan C},
doi = {10.1109/MSP.2008.930649},
isbn = {1053-5888},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
pmid = {15700530},
title = {{Mean Squared Error : Love It or Leave It ?}},
year = {2009}
}
@article{Wang2002,
abstract = {Image quality assessment plays an important role in various image processing applications. A great deal of effort has been made in re- cent years to develop objective image quality metrics that correlate with perceived quality measurement. Unfortunately, only limited success has been achieved. In this paper, we provide some insights on why image quality assessment is so difficult by pointing out the weaknesses of the error sensitivity based framework, which has been used by most image quality assessment approaches in the lit- erature. Furthermore, we propose a new philosophy in designing im- age quality metrics: The main function of the human eyes is to extract structural information from the viewing field, and the hu- man visual system is highly adapted for this purpose. Therefore, a measurement of structural distortion should be a good approxima- tion of perceived image distortion. Based on the new philosophy, we implemented a simple but effective image quality indexing al- gorithm, which is very promising as shown by our current results.},
author = {Wang, Zhou and Bovik, Alan C. and Lu, Ligang},
doi = {10.1109/ICASSP.2002.5745362},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
title = {{Why is image quality assessment so difficult?}},
year = {2002}
}
@article{Wang2002a,
abstract = {We propose a new universal objective image quality index, which is easy to calculate and applicable to various image processing applications. Instead of using traditional error summation methods, the proposed index is designed by modeling any image distortion as a combination of three factors: loss of correlation, luminance distortion, and contrast distortion. Although the new index is mathematically defined and no human visual system model is explicitly employed, our experiments on various image distortion types indicate that it performs significantly better than the widely used distortion metric mean squared error. Demonstrative images and an efficient MATLAB implementation of the algorithm are available online at http://anchovy.ece.utexas.edu//spl sim/zwang/research/quality{\_}index/demo.html.},
author = {Wang, Zhou and Bovik, Alan C.},
doi = {10.1109/97.995823},
isbn = {1070-9908},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {Human visual system (HVS),Image quality measurement,Mean squared error (MSE)},
title = {{A universal image quality index}},
year = {2002}
}
@article{Xue2014,
abstract = {this paper proposed an image quality assessment metric, Gradient based. every easy! just calculate the gradient of reference image and current image. and use average pooling to get a gradient mean value. then calculate the standard deviation. There are three aspects to evaluate the performance of IQA model. prediction accuracy prediction monotonicity and prediction consistency.},
author = {Xue, Wufeng and Zhang, Lei and Mou, Xuanqin and Bovik, Alan C.},
doi = {10.1109/TIP.2013.2293423},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {GMSD,Gradient magnitude similarity,Xue2014,full reference,image quality assessment,standard deviation pooling},
mendeley-tags = {GMSD,Xue2014},
number = {2},
pages = {668--695},
title = {{Gradient magnitude similarity deviation: A highly efficient perceptual image quality index}},
volume = {23},
year = {2014}
}
@article{Ruderman1994,
abstract = {Recently there has been a resurgence of interest in the properties of natural images. Their statistics are important not only in image compression but also far the study of sensory processing in biology, which can be viewed as satisfying cettain design criteria. This review summarizes previous work on image statistics and presents our own data Perhaps the most notable property of natural images is an invariance to scale. We present data to support this claim as well 35 evidence for a hierarchical invariance in natural scenes. These symmetries provide a powerful description of natunl images as they g{\~{}}dy d i s Vib ut ions. resttiit the class of allowed},
author = {Ruderman, Daniel L.},
doi = {10.1088/0954-898X_5_4_006},
isbn = {0954-898X},
issn = {0954898X},
journal = {Network: Computation in Neural Systems},
number = {4},
pages = {517--548},
title = {{The statistics of natural images}},
volume = {5},
year = {1994}
}
@article{Liu2014a,
abstract = {We study the efficacy of utilizing a powerful image descriptor, the curvelet transform, to learn a no-reference (NR) image quality assessment (IQA) model. A set of statistical features are extracted from a computed image curvelet representation, including the coordinates of the maxima of the log-histograms of the curvelet coefficients values, and the energy distributions of both orientation and scale in the curvelet domain. Our results indicate that these features are sensitive to the presence and severity of image distortion. Operating within a 2-stage framework of distortion classification followed by quality assessment, we train an image distortion and quality prediction engine using a support vector machine (SVM). The resulting algorithm, dubbed CurveletQA for short, was tested on the LIVE IQA database and compared to state-of-the-art NR/FR IQA algorithms. We found that CurveletQA correlates well with human subjective opinions of image quality, delivering performance that is competitive with popular full-reference (FR) IQA algorithms such as SSIM, and with top-performing NR IQA models. At the same time, CurveletQA has a relatively low complexity. {\textcopyright} 2014 Elsevier B.V.},
author = {Liu, Lixiong and Dong, Hongping and Huang, Hua and Bovik, Alan C.},
doi = {10.1016/j.image.2014.02.004},
file = {::},
isbn = {0923-5965},
issn = {09235965},
journal = {Signal Processing: Image Communication},
keywords = {Curvelet,Curvelet-QA,Feature-SVR,Image Quality,Image quality assessment (IQA),Natural scene statistics (NSS),No Reference,No reference (NR),Support Vector Machine (SVM)},
mendeley-groups = {IQA-NR},
mendeley-tags = {Curvelet-QA,Feature-SVR,Image Quality,No Reference},
number = {4},
pages = {494--505},
title = {{No-reference image quality assessment in curvelet domain}},
volume = {29},
year = {2014}
}
@book{Gonzalez2008,
abstract = {For courses in Image Processing and Computer Vision. Completely self-contained-and heavily illustrated-this introduction to basic concepts and methodologies for digital image processing is written at a level that truly is suitable for seniors and first-year graduate students in almost any technical discipline. The leading textbook in its field for more than twenty years, it continues its cutting-edge focus on contemporary developments in all mainstream areas of image processing-e.g., image fundamentals, image enhancement in the spatial and frequency domains, restoration, color image processing, wavelets, image compression, morphology, segmentation, image description, and the fundamentals of object recognition. It focuses on material that is fundamental and has a broad scope of application.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gonzalez, Rafael C.. and Woods, Richard Eugene},
booktitle = {Nueva Jersey},
doi = {10.1049/ep.1978.0474},
eprint = {arXiv:1011.1669v3},
isbn = {0132345633},
issn = {10833668},
pages = {976},
pmid = {21707695},
title = {{Digital image processing}},
url = {https://books.google.com/books?id=lDojQwAACAAJ{\&}pgis=1},
year = {2008}
}
@article{Sheikh2005,
abstract = {Measurement of visual quality is of fundamental importance to numerous image and video processing applications. The goal of quality assessment (QA) research is to design algorithms that can automatically assess the quality of images or videos in a perceptually consistent manner. Traditionally, image QA algorithms interpret image quality as fidelity or similarity with a "reference" or "perfecft" image in some perceptual space. Such "full-referenc" QA methods attempt to achieve consistency in quality prediction by modeling salient physiological and psychovisual features of the human visual system (HVS), or by arbitrary signal fidelity criteria. In this paper, we approach the problem of image QA by proposing a novel information fidelity criterion that is based on natural scene statistics. QA systems are invariably involved with judging the visual quality of images and videos that are meant for "human consumption." Researchers have developed sophisticated models to capture the statistics of natural signals, that is, pictures and videos of the visual environment. Using these statistical models in an information-theoretic setting, we derive a novel QA algorithm that provides clear advantages over the traditional approaches. In particular, it is parameterless and outperforms current methods in our testing. We validate the performance of our algorithm with an extensive subjective study involving 779 images. We also show that, although our approach distinctly departs from traditional HVS-based methods, it is functionally similar to them under certain conditions, yet it outperforms them due to improved modeling. The code and the data from the subjective study are available at.},
author = {Sheikh, Hamid Rahim and Bovik, Alan Conrad and de Veciana, Gustavo},
doi = {10.1109/TIP.2005.859389},
isbn = {1057-7149},
issn = {1057-7149},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
number = {12},
pages = {2117--2128},
pmid = {16370464},
title = {{An information fidelity criterion for image quality assessment using natural scene statistics.}},
volume = {14},
year = {2005}
}
@article{Sheikh2006,
abstract = {Measurement of visual quality is of fundamental importance to numerous image and video processing applications. The goal of quality assessment (QA) research is to design algorithms that can automatically assess the quality of images or videos in a perceptually consistent manner. Image QA algorithms generally interpret image quality as fidelity or similarity with a "reference" or "perfect" image in some perceptual space. Such "full-reference" QA methods attempt to achieve consistency in quality prediction by modeling salient physiological and psychovisual features of the human visual system (HVS), or by signal fidelity measures. In this paper, we approach the image QA problem as an information fidelity problem. Specifically, we propose to quantify the loss of image information to the distortion process and explore the relationship between image information and visual quality. QA systems are invariably involved with judging the visual quality of "natural" images and videos that are meant for "human consumption." Researchers have developed sophisticated models to capture the statistics of such natural signals. Using these models, we previously presented an information fidelity criterion for image QA that related image quality with the amount of information shared between a reference and a distorted image. In this paper, we propose an image information measure that quantifies the information that is present in the reference image and how much of this reference information can be extracted from the distorted image. Combining these two quantities, we propose a visual information fidelity measure for image QA. We validate the performance of our algorithm with an extensive subjective study involving 779 images and show that our method outperforms recent state-of-the-art image QA algorithms by a sizeable margin in our simulations. The code and the data from the subjective study are available at the LIVE website.},
author = {Sheikh, H.R. Hamid Rahim and Bovik, Alan C. A.C.},
doi = {10.1109/TIP.2005.859378},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Image formation,Image quality assessment (QA),Information fidelity,Natural scene statistics (NSS)},
number = {2},
pages = {430--444},
pmid = {16479813},
title = {{Image information and visual quality}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1576816},
volume = {15},
year = {2006}
}
@article{Zhang2014a,
abstract = {Perceptual image quality assessment (IQA) aims to use computational models to measure the image quality in consistent with subjective evaluations. Visual saliency (VS) has been widely studied by psychologists, neurobiologists, and computer scientists during the last decade to investigate, which areas of an image will attract the most attention of the human visual system. Intuitively, VS is closely related to IQA in that suprathreshold distortions can largely affect VS maps of images. With this consideration, we propose a simple but very effective full reference IQA method using VS. In our proposed IQA model, the role of VS is twofold. First, VS is used as a feature when computing the local quality map of the distorted image. Second, when pooling the quality score, VS is employed as a weighting function to reflect the importance of a local region. The proposed IQA index is called visual saliency-based index (VSI). Several prominent computational VS models have been investigated in the context of IQA and the best one is chosen for VSI. Extensive experiments performed on four large-scale benchmark databases demonstrate that the proposed IQA index VSI works better in terms of the prediction accuracy than all state-of-the-art IQA indices we can find while maintaining a moderate computational complexity. The MATLAB source code of VSI and the evaluation results are publicly available online at http://sse.tongji.edu.cn/linzhang/IQA/VSI/VSI.htm.},
author = {Zhang, Lin and Shen, Ying and Li, Hongyu},
doi = {10.1109/TIP.2014.2346028},
isbn = {1057-7149 VO - 23},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Full Reference,Lin2014,Perceptual image quality assessment,VSI2014,visual saliency},
mendeley-tags = {Full Reference,Lin2014,VSI2014},
number = {10},
pages = {4270--4281},
pmid = {25122572},
title = {{VSI: A visual saliency-induced index for perceptual image quality assessment}},
volume = {23},
year = {2014}
}
@article{Mansouri2009,
abstract = {In objective image quality metrics, one of the most important factors is the correlation of their results with the perceived quality measurements. In this paper, a new method is presented based on comparing between the structural properties of the two compared images. Based on the mathematical concept of the singular value decomposition (SVD) theorem, each matrix can be factorized to the products of three matrices, one of them related to the luminance value while the two others show the structural content information of the image. A new method to quantify the quality of images is proposed based on the projected coefficients and the left singular vector matrix of the disturbed image based on the right singular vector matrix of the original image. To evaluate this performance, many tests have been done using a widespread subjective study involving 779 images of the Live Image Quality Assessment Database, Release 2005. The objective results show a high rate of correlation with subjective quality measurements. {\textcopyright} 2009 The Optical Society of Japan.},
author = {Mansouri, Azadeh and Aznaveh, Ahmad Mahmoudi and Torkamani-Azar, Farah and Jahanshahi, J. Afshar},
doi = {10.1007/s10043-009-0010-y},
issn = {13406000},
journal = {Optical Review},
keywords = {Image processing,Image quality,Matrix algebra,Matrix decomposition and singular value decomposit},
title = {{Image quality assessment using the singular value decomposition theorem}},
year = {2009}
}
@article{Mansouri2019,
abstract = {In the last decade, some impressive image quality metrics have been proposed; however, designing an image quality metric which predicts human judgments is still a challenging issue. It is due to the complexity of the human visual system. Singular value decomposition (SVD), as a useful tool, has been employed for evaluating the perceptual quality of visual information. The efficiency of the SVD-based image quality assessment (IQA) methods is related to its ability to extract the structural information of the viewing scene. In this paper, a new SVD-based IQA method is presented in which the structural information of the distorted image is evaluated based on its reflection on the original singular vector matrices. The experimental results show that the proposed algorithm can effectively evaluate the natural image quality in a consistent manner with the human visual perception.},
author = {Mansouri, Azadeh and Mahmoudi-Aznaveh, Ahmad},
doi = {10.1016/j.image.2019.01.007},
issn = {09235965},
journal = {Signal Processing: Image Communication},
keywords = {Human visual system,Image quality assessment,SSVD,Singular value decomposition(SVD)},
title = {{SSVD: Structural SVD-based image quality assessment}},
year = {2019}
}
@article{Chandler2007,
abstract = {This paper presents an efficient metric for quantifying the visual fidelity of natural images based on near-threshold and suprathreshold properties of human vision. The proposed metric, the visual signal-to-noise ratio (VSNR), operates via a two-stage approach. In the first stage, contrast thresholds for detection of distortions in the presence of natural images are computed via wavelet-based models of visual masking and visual summation in order to determine whether the distortions in the distorted image are visible. If the distortions are below the threshold of detection, the distorted image is deemed to be of perfect visual fidelity (VSNR = infinity) and no further analysis is required. If the distortions are suprathreshold, a second stage is applied which operates based on the low-level visual property of perceived contrast, and the mid-level visual property of global precedence. These two properties are modeled as Euclidean distances in distortion-contrast space of a multiscale wavelet decomposition, and VSNR is computed based on a simple linear sum of these distances. The proposed VSNR metric is generally competitive with current metrics of visual fidelity; it is efficient both in terms of its low computational complexity and in terms of its low memory requirements; and it operates based on physical luminances and visual angle (rather than on digital pixel values and pixel-based dimensions) to accommodate different viewing conditions.},
archivePrefix = {arXiv},
arxivId = {1607.08822},
author = {Chandler, Damon M. and Hemami, Sheila S.},
doi = {10.1109/TIP.2007.901820},
eprint = {1607.08822},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Contrast,Distortion,Human visual system (HVS),Image fidelity,Image quality,Noise,Visual fidelity,Wavelet},
pmid = {17784602},
title = {{VSNR: A wavelet-based visual signal-to-noise ratio for natural images}},
year = {2007}
}
@article{Zhang2011a,
abstract = {Image quality assessment (IQA) aims to use computational models to measure the image quality consistently with subjective evaluations. The well-known structural similarity index brings IQA from pixel- to structure-based stage. In this paper, a novel feature similarity (FSIM) index for full reference IQA is proposed based on the fact that human visual system (HVS) understands an image mainly according to its low-level features. Specifically, the phase congruency (PC), which is a dimensionless measure of the significance of a local structure, is used as the primary feature in FSIM. Considering that PC is contrast invariant while the contrast information does affect HVS' perception of image quality, the image gradient magnitude (GM) is employed as the secondary feature in FSIM. PC and GM play complementary roles in characterizing the image local quality. After obtaining the local quality map, we use PC again as a weighting function to derive a single quality score. Extensive experiments performed on six benchmark IQA databases demonstrate that FSIM can achieve much higher consistency with the subjective evaluations than state-of-the-art IQA metrics.},
author = {Zhang, Lin and Zhang, Lei and Mou, Xuanqin and Zhang, David},
doi = {10.1109/TIP.2011.2109730},
file = {::},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Color Component,Full Reference,Gradient,Image Quality,image quality assessment (IQA),low-level feature,phase congruency (PC)},
mendeley-groups = {Color Component,IQA-FR},
mendeley-tags = {Color Component,Full Reference,Image Quality},
number = {8},
pages = {2378--2386},
pmid = {21292594},
title = {{FSIM: A feature similarity index for image quality assessment}},
volume = {20},
year = {2011}
}
@misc{Vapnik1995,
abstract = {A very good high-level introduction in Statistical Learning Theory in the VC formulation, plus a comprehensive overview of the SV algorithm. First description of SV machines for regression estimation.},
author = {Vapnik, V N},
booktitle = {Springer},
doi = {10.1109/TNN.1997.641482},
isbn = {0387945598},
issn = {10459227},
pages = {188},
pmid = {18255760},
title = {{The Nature of Statistical Learning Theory}},
url = {http://portal.acm.org/citation.cfm?id=211359},
volume = {8},
year = {1995}
}
@article{Mittal2012a,
abstract = {We propose a natural scene statistic-based distortion-generic blind/no-reference (NR) image quality assessment (IQA) model that operates in the spatial domain. The new model, dubbed blind/referenceless image spatial quality evaluator (BRISQUE) does not compute distortion-specific features, such as ringing, blur, or blocking, but instead uses scene statistics of locally normalized luminance coefficients to quantify possible losses of "naturalness" in the image due to the presence of distortions, thereby leading to a holistic measure of quality. The underlying features used derive from the empirical distribution of locally normalized luminances and products of locally normalized luminances under a spatial natural scene statistic model. No transformation to another coordinate frame (DCT, wavelet, etc.) is required, distinguishing it from prior NR IQA approaches. Despite its simplicity, we are able to show that BRISQUE is statistically better than the full-reference peak signal-to-noise ratio and the structural similarity index, and is highly competitive with respect to all present-day distortion-generic NR IQA algorithms. BRISQUE has very low computational complexity, making it well suited for real time applications. BRISQUE features may be used for distortion-identification as well. To illustrate a new practical application of BRISQUE, we describe how a nonblind image denoising algorithm can be augmented with BRISQUE in order to perform blind image denoising. Results show that BRISQUE augmentation leads to performance improvements over state-of-the-art methods. A software release of BRISQUE is available online: http://live.ece.utexas.edu/research/quality/BRISQUE{\_}release.zip for public use and evaluation.},
author = {Mittal, Anish and Moorthy, Anush Krishna and Bovik, Alan Conrad},
doi = {10.1109/TIP.2012.2214050},
file = {::},
isbn = {1057-7149},
issn = {1941-0042},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
keywords = {BRISQUE,Feature-SVR,Image Quality,Mittal2012,No Reference},
mendeley-groups = {IQA-NR},
mendeley-tags = {BRISQUE,Feature-SVR,Image Quality,Mittal2012,No Reference},
number = {12},
pages = {4695--708},
pmid = {22910118},
title = {{No-reference image quality assessment in the spatial domain.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22910118},
volume = {21},
year = {2012}
}
@article{sharifi1995estimation,
author = {Sharifi, Karnran and Leon-Garcia, Alberto},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
number = {1},
pages = {52--56},
publisher = {IEEE},
title = {{Estimation of shape parameter for generalized Gaussian distributions in subband decompositions of video}},
volume = {5},
year = {1995}
}
@article{Moorthy2010,
abstract = {Present day no-reference/no-reference image quality assessment (NR IQA) algorithms usually assume that the distortion affecting the image is known. This is a limiting assumption for practical applications, since in a majority of cases the distortions in the image are unknown. We propose a new two-step framework for no-reference image quality assessment based on natural scene statistics (NSS). Once trained, the framework does not require any knowledge of the distorting process and the framework is modular in that it can be extended to any number of distortions. We describe the framework for blind image quality assessment and a version of this framework-the blind image quality index (BIQI) is evaluated on the LIVE image quality assessment database. A software release of BIQI has been made available online: http://live.ece.utexas.edu/research/quality/BIQI{\_}release.zip.},
author = {Moorthy, Anush Krishna and Bovik, Alan Conrad},
doi = {10.1109/LSP.2010.2043888},
file = {::},
isbn = {1070-9908},
issn = {10709908},
journal = {IEEE Signal Processing Letters},
keywords = {BIQI,Feature-SVR,Image Quality,Moorthy2010,No Reference},
mendeley-groups = {IQA-NR},
mendeley-tags = {BIQI,Feature-SVR,Image Quality,Moorthy2010,No Reference},
number = {5},
pages = {513--516},
title = {{A two-step framework for constructing blind image quality indices}},
volume = {17},
year = {2010}
}
@article{Saad2012,
abstract = {We develop an efficient, general-purpose, blind/noreference image quality assessment (NR-IQA) algorithm using a natural scene statistics (NSS) model of discrete cosine transform (DCT) coefficients. The algorithm is computationally appealing, given the availability of platforms optimized for DCT computation. The approach relies on a simple Bayesian inference model to predict image quality scores given certain extracted features. The features are based on an NSS model of the image DCT coefficients. The estimated parameters of the model are utilized to form features that are indicative of perceptual quality. These features are used in a simple Bayesian inference approach to predict quality scores. The resulting algorithm, which we name BLIINDS-II, requires minimal training and adopts a simple probabilistic model for score prediction. Given the extracted features from a test image, the quality score that maximizes the probability of the empirically determined inference model is chosen as the predicted quality score of that image. When tested on the LIVE IQA database, BLIINDS-II is shown to correlate highly with human judgments of quality, at a level that is competitive with the popular SSIM index.},
author = {Saad, Michele a and Bovik, Alan C and Charrier, Christophe},
doi = {10.1109/TIP.2012.2191563},
file = {:C$\backslash$:/Users/PersianNB/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saad, Bovik, Charrier - 2012 - Blind image quality assessment a natural scene statistics approach in the DCT domain.pdf:pdf},
isbn = {1057-7149},
issn = {1941-0042},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,BLIINDSII,Computer-Assisted,Computer-Assisted: methods,Data Interpretation,Feature-SVR,Image Enhancement,Image Enhancement: methods,Image Interpretation,Image Quality,Imaging,No Reference,Pattern Recognition,Reproducibility of Results,Saad2012,Sensitivity and Specificity,Single-Blind Method,Statistical,Three-Dimensional,Three-Dimensional: methods},
mendeley-groups = {IQA-NR},
mendeley-tags = {BLIINDSII,Feature-SVR,Image Quality,No Reference,Saad2012},
number = {8},
pages = {3339--52},
pmid = {22453635},
title = {{Blind image quality assessment: a natural scene statistics approach in the DCT domain.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22453635},
volume = {21},
year = {2012}
}
@article{Gupta2018,
abstract = {We develop a Generalized Gaussian scale mixture (GGSM) model of the wavelet coefficients of natural and distorted images. The GGSM model, which is more general than and which subsumes the Gaussian scale mixture (GSM) model, is shown to be a better representation of the statistics of the wavelet coefficients of both natural as well as distorted images. We demonstrate the utility of the model by applying it to various image processing applications, including blind distortion identification and no reference image quality assessment (NR-IQA). Similar to the GSM model, the GGSM model is useful for motivating the use of local divisive energy normalization, especially when the wavelet coefficients are computed on distorted pictures. We show that the GGSM model can lead to improved performance in distortion-related applications, while providing a more principled approach to the statistical processing of distorted image signals. The software release of a GGSM-based NR-IQA approach called DIIVINE-GGSM is available online at http://live.ece.utexas.edu/research/quality/diivine-ggsm.zip for further experimentation.},
author = {Gupta, Praful and Moorthy, Anush Krishna and Soundararajan, Rajiv and Bovik, Alan Conrad},
doi = {10.1016/J.IMAGE.2018.05.009},
issn = {0923-5965},
journal = {Signal Processing: Image Communication},
mendeley-groups = {2- MLIVE-cited},
month = {aug},
pages = {87--94},
publisher = {Elsevier},
title = {{Generalized Gaussian scale mixtures: A model for wavelet coefficients of natural images}},
url = {https://www.sciencedirect.com/science/article/pii/S0923596518303710},
volume = {66},
year = {2018}
}
@article{Badrinarayanan2017,
abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN and also with the well known DeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. We show that SegNet provides good performance with competitive inference time and more efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
archivePrefix = {arXiv},
arxivId = {1511.00561},
author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
doi = {10.1109/TPAMI.2016.2644615},
eprint = {1511.00561},
isbn = {9783319464879},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Deep convolutional neural networks,cnn,decoder,deep learninng,encoder,indoor scenes,pooling,road scenes,semantic pixel-wise segmentation,upsampling},
mendeley-tags = {cnn,deep learninng},
number = {12},
pages = {2481--2495},
pmid = {28060704},
title = {{SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}},
volume = {39},
year = {2017}
}
@misc{Ma2017a,
author = {Ma, Kede and Liu, Wentao and Zhang, Kai and Duanmu, Zhengfang and Wang, Zhou and Zuo, Wangmeng},
booktitle = {IEEE Transactions on Image Processing},
doi = {10.1109/TIP.2017.2774045},
file = {::},
issn = {10577149},
keywords = {Blind image quality assessment,Image Quality,Image coding,Image quality,MEON,Ma2018,Neural networks,No Reference,Nonlinear distortion,Training,cnn,deep learninng,deep neural networks,gMAD competition,generalized divisive normalization,multi-task learning},
mendeley-groups = {IQA-NR},
mendeley-tags = {Image Quality,MEON,Ma2018,No Reference,cnn,deep learninng},
title = {{End-to-End Blind Image Quality Assessment Using Deep Neural Networks}},
year = {2017}
}
@article{liu2017rankiqa,
author = {Liu, Xialei and van de Weijer, Joost and Bagdanov, Andrew D},
file = {::},
journal = {arXiv preprint arXiv:1707.08347},
keywords = {Image Quality,Liu2017,No Reference,RankIQA},
mendeley-groups = {IQA-NR,2- MLIVE-cited},
mendeley-tags = {Image Quality,Liu2017,No Reference,RankIQA},
title = {{RankIQA: Learning from Rankings for No-reference Image Quality Assessment}},
year = {2017}
}
@inproceedings{Jurie2005,
abstract = {Visual codebook based quantization of robust appearance descriptors extracted from local image patches is an effective means of capturing image statistics for texture analysis and scene classification. Codebooks are usually constructed by using a method such as k-means to cluster the descriptor vectors of patches sampled either densely ('textons') or sparsely ('bags of features' based on keypoints or salience measures) from a set of training images. This works well for texture analysis in homogeneous images, but the images that arise in natural object recognition tasks have far less uniform statistics. We show that for dense sampling, k-means over-adapts to this, clustering centres almost exclusively around the densest few regions in descriptor space and thus failing to code other informative regions. This gives suboptimal codes that are no better than using randomly selected centres. We describe a scalable acceptance-radius based clusterer that generates better codebooks and study its performance on several image classification tasks. We also show that dense representations outperform equivalent keypoint based ones on these tasks and that SVM or Mutual Information based feature selection starting from a dense codebook further improves the performance. {\textcopyright} 2005 IEEE.},
author = {Jurie, Frederic and Triggs, Bill},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2005.66},
isbn = {076952334X},
title = {{Creating efficient codebooks for visual recognition}},
year = {2005}
}
@article{Ye2012b,
abstract = {In this paper, we present an efficient general-purpose objective $\backslash$nno-reference (NR) image quality assessment (IQA) framework based on unsupervised $\backslash$nfeature learning. The goal is to build a computational model to automatically $\backslash$npredict human perceived image quality without a reference image and without $\backslash$nknowing the distortion present in the image. Previous approaches for this $\backslash$nproblem typically rely on hand-crafted features which are carefully designed $\backslash$nbased on prior knowledge. In contrast, we use raw-image-patches extracted from a $\backslash$nset of unlabeled images to learn a dictionary in an unsupervised manner. We use $\backslash$nsoft-assignment coding with max pooling to obtain effective image $\backslash$nrepresentations for quality estimation. The proposed algorithm is very $\backslash$ncomputationally appealing, using raw image patches as local descriptors and $\backslash$nusing soft-assignment for encoding. Furthermore, unlike previous methods, our $\backslash$nunsupervised feature learning strategy enables our method to adapt to different $\backslash$ndomains. CORNIA (Codebook Representation for No-Reference Image Assessment) is $\backslash$ntested on LIVE database and shown to perform statistically better than the $\backslash$nfull-reference quality measure, structural similarity index (SSIM) and is shown $\backslash$nto be comparable to state-of-the-art general purpose NR-IQA algorithms.},
author = {Ye, Peng and Kumar, J and Kang, Le and Doermann, D},
doi = {10.1109/CVPR.2012.6247789},
file = {::},
isbn = {1063-6919 VO -},
issn = {1063-6919},
journal = {Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on},
keywords = {CORNIA,Feature-SVR,No Reference,dictionaries,encoding,feature extraction,image cod},
mendeley-groups = {IQA-NR},
mendeley-tags = {CORNIA,Feature-SVR,No Reference},
pages = {1098--1105},
title = {{Unsupervised feature learning framework for no-reference image quality assessment}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6247789},
year = {2012}
}
@article{Xu2016,
abstract = {Blind image quality assessment (BIQA) research aims to develop a perceptual model to evaluate the quality of distorted images automatically and accurately without access to the non-distorted reference images. The state-of-the-art general purpose BIQA methods can be classified into two categories according to the types of features used. The first includes handcrafted features which rely on the statistical regularities of natural images. These, however, are not suitable for images containing text and artificial graphics. The second includes learning-based features which invariably require large codebook or supervised codebook updating procedures to obtain satisfactory performance. These are time-consuming and not applicable in practice. In this paper, we propose a novel general purpose BIQA method based on high order statistics aggregation (HOSA), requiring only a small codebook. HOSA consists of three steps. First, local normalized image patches are extracted as local features through a regular grid, and a codebook containing 100 codewords is constructed by K-means clustering. In addition to the mean of each cluster, the diagonal covariance and coskewness (i.e., dimension-wise variance and skewness) of clusters are also calculated. Second, each local feature is softly assigned to several nearest clusters and the differences of high order statistics (mean, variance and skewness) between local features and corresponding clusters are softly aggregated to build the global quality aware image representation. Finally, support vector regression is adopted to learn the mapping between perceptual features and subjective opinion scores. The proposed method has been extensively evaluated on ten image databases with both simulated and realistic image distortions, and shows highly competitive performance to the state-of-the-art BIQA methods.},
author = {Xu, Jingtao and Ye, Peng and Li, Qiaohong and Du, Haiqing and Liu, Yong and Doermann, David},
doi = {10.1109/TIP.2016.2585880},
file = {::},
isbn = {9781467399616},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Blind image quality assessment,Feature-SVR,HOSA,Image Quality,No Reference,codebook,feature normalization,high order statistics,local feature aggregation,support vector regression},
mendeley-groups = {IQA-NR,2- MLIVE-cited},
mendeley-tags = {Feature-SVR,HOSA,Image Quality,No Reference},
number = {9},
pages = {4444--4457},
title = {{Blind image quality assessment based on high order statistics aggregation}},
volume = {25},
year = {2016}
}
@article{Zhang2014,
abstract = {With the rapid development of the usage of digital imaging and communication technologies, there appears to be a great demand for fast and practical approaches for image quality assessment (IQA) algorithms that can match human judge- ments. In this paper, we propose a novel general-purpose no-reference IQA (NR-IQA) framework by means of learn- ing quality-aware filters (QAF). Using these filters for im- age encoding, we can obtain effective image representations for quality estimation. Additionally, random forest is used to learn the mapping from feature space to human subjec- tive scores. Extensive experiments conducted on LIVE and CSIQ databases demonstrate that the proposed NR-IQA met- ric QAF can achieve better prediction performance than all the other state-of-the-art NR-IQA approaches in terms of both prediction accuracy and generalization capabilities},
author = {Zhang, Lin and Gu, Zhongyi and Liu, Xiaoxu and Li, Hongyu and Lu, Jianwei},
doi = {10.1109/MMUL.2014.50},
file = {::},
isbn = {978-1-4799-4761-4},
issn = {1070986X},
journal = {IEEE Multimedia},
keywords = {Digital imaging,Feature extraction,Feature-SVR,Filtering,Image Quality,Image coding,Image quality,Multimedia,NR-IQA,Natural scene statistics,No Reference,No-reference image quality assessment,Predictive models,QAF,Quality-aware filters,Random forest,Research and development,Sparse filtering,Training,Zhang2014},
mendeley-groups = {IQA-NR},
mendeley-tags = {Feature-SVR,Image Quality,No Reference,QAF,Zhang2014},
number = {4},
pages = {67--75},
title = {{Training quality-aware filters for no-reference image quality assessment}},
volume = {21},
year = {2014}
}
@inproceedings{Zhang2015b,
abstract = {non-reference image      . signal level semantic sense   quality  (image quality    quality   )},
author = {Zhang, Peng and Zhou, Wengang and Wu, Lei and Li, Houqiang},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298853},
file = {::},
isbn = {9781467369640},
issn = {10636919},
keywords = {Image Quality,No Reference,SOM,Zhang2015},
mendeley-groups = {IQA-NR},
mendeley-tags = {Image Quality,No Reference,SOM,Zhang2015},
pages = {2394--2402},
title = {{SOM: Semantic obviousness metric for image quality assessment}},
volume = {07-12-June},
year = {2015}
}
@inproceedings{Xue2013,
abstract = {General purpose blind image quality assessment (BIQA) has been recently attracting significant attention in the fields of image processing, vision and machine learning. State-of-the-art BIQA methods usually learn to evaluate the image quality by regression from human subjective scores of the training samples. However, these methods need a large number of human scored images for training, and lack an explicit explanation of how the image quality is affected by image local features. An interesting question is then: can we learn for effective BIQA without using human scored images? This paper makes a good effort to answer this question. We partition the distorted images into overlapped patches, and use a percentile pooling strategy to estimate the local quality of each patch. Then a quality-aware clustering (QAC) method is proposed to learn a set of centroids on each quality level. These centroids are then used as a codebook to infer the quality of each patch in a given image, and subsequently a perceptual quality score of the whole image can be obtained. The proposed QAC based BIQA method is simple yet effective. It not only has comparable accuracy to those methods using human scored images in learning, but also has merits such as high linearity to human perception of image quality, real-time implementation and availability of image local quality map. 2013 IEEE.},
author = {Xue, Wufeng and Zhang, Lei and Mou, Xuanqin},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2013.133},
file = {::},
isbn = {10636919},
issn = {10636919},
keywords = {Feature-SVR,Image Quality,NRQI,No Reference,QAC,Xue2013,bind image quality assessment,clustering,qualiyt aware},
mendeley-groups = {IQA-NR},
mendeley-tags = {Feature-SVR,Image Quality,NRQI,No Reference,QAC,Xue2013},
pages = {995--1002},
title = {{Learning without human scores for blind image quality assessment}},
year = {2013}
}
@article{goodman1979multidimensional,
author = {Goodman, J S and Pearson, D E},
journal = {IEEE Trans Syst Man Cybern},
number = {6},
pages = {353--356},
title = {{Multidimensional scaling of multiply-impaired television pictures}},
volume = {9},
year = {1979}
}
@article{linde1981similarity,
author = {Linde, L},
journal = {FOA Rep. C},
number = {53004-H9},
publisher = {Swedish National Defense Research Institute},
title = {{Similarity of distorted pictures: on the interaction between edge blur and random noise}},
year = {1981}
}
@article{Kayargadde1996,
abstract = {Reliable and economic methods for assessing image quality are essential for designing better imaging systems. Although reliable psychophysical methods are available for assessing perceptual image quality with the help of human subjects, the cost of performing such experiments prevents their use for evaluating large amounts of image material. This has led to an increasing demand for objective methods for estimating image quality. The perceived quality of an image is usually determined by several underlying perceptual attributes such as sharpness and noisiness. In the accompanying paper [J. Opt. Soc. Am. A 13, 1166-1177 (1996)] it is demonstrated that the relationships between images on the one hand and judgments on attributes and overall quality by subjects on the other hand can be characterized in a multidimensional perceptual space. In this perceptual space the images are represented by points, and the strengths of their perceptual attributes are modeled by the projections of these image positions onto the attribute axes. In analogy with the perceptual space we will introduce a psychometric space in which the positions of the images are determined by objective measures on the images. In the case of images degraded by blur and noise the stimulus coordinates are functions of the estimated spread of the blurring kernel and the estimated standard deviation of the noise, respectively. According to the model presented in this paper, the perceptual attributes of images can be estimated in three steps. In the first step the physical parameters (blur spread and noise standard deviation) are estimated from the images. In the second step these estimates are used to position the images in psychometric space. In the third step the attribute strengths are derived by projecting the alter image positions onto the attribute axes. We show that the attributes and the quality thus estimated correlate well with the perceived attributes and quality.},
author = {Kayargadde, Vishwakumara and Martens, Jean-Bernard},
doi = {10.1364/JOSAA.13.001178},
isbn = {1084-7529 (Print)},
issn = {1084-7529},
journal = {Journal of the Optical Society of America A},
keywords = {Multiple Distortions},
mendeley-groups = {MD},
mendeley-tags = {Multiple Distortions},
number = {6},
pages = {1178},
pmid = {8926548},
title = {{Perceptual characterization of images degraded by blur and noise: model}},
url = {https://www.osapublishing.org/abstract.cfm?URI=josaa-13-6-1178},
volume = {13},
year = {1996}
}
@inproceedings{Reenu2013,
abstract = {This letter presents a wavelet based perceptive image quality assessment metric (WASH-WAvelet based SHarp features) based on HVS which accounts for the sensitivity of human vision to sharp features of the image, the sharpness and zero-crossings. The image is analyzed in the wavelet domain which is advantageous to the quality assessment based on human perception. The sharp regions are highly attentive to early vision and the edgepoints obtained by zero-crossing are important features that give good quality estimation based on the structural distortion of the image. The WASH algorithm was tested on the publicly available databases and gave best results with the state-of-the-art quality assessment metrics as well as the wavelet based perceptive quality assessment metrics. {\textcopyright} 2013 IEEE.},
author = {Reenu, M. and David, Dayana and {Aneesh Raj}, S. S. and Nair, Madhu S.},
booktitle = {Proceedings - 2nd International Conference on Advanced Computing, Networking and Security, ADCONS 2013},
doi = {10.1109/ADCONS.2013.25},
isbn = {9780768551272},
keywords = {HVS,Image quality assessment,Sharpness,Wavelet,Zero-crossing},
title = {{Wavelet based sharp features (WASH): An image quality assessment metric based on HVS}},
year = {2013}
}
@article{Okarma2014,
abstract = {In this paper the problem of quality assessment of images containing various types of distortions is concerned. Many image quality metrics proposed during last decade are quite well correlated with human perception of various kinds of distortions with the assumption that only a single type of distortions is present in the image. One of the main reasons of such approach is the lack of datasets containing subjective quality assessment results of multiply distorted images. However, after the development of LIVE Multiply Distorted Image Quality Database, a new challenge related to verification of usability of known metrics as well as the development of new ones has appeared. In this paper, the results of such verification is presented not only for some well-known metrics but also for recently proposed combined metrics together with the proposed new combined metrics optimized for multiply distorted images. The new metrics outperform previously proposed ones in the aspect of linear correlation with subjective evaluations of images containing multiple distortions. DOI: http://dx.doi.org/10.5755/j01.eee.20.6.7284},
author = {Okarma, K. and K.},
doi = {10.5755/j01.eee.20.6.7284},
file = {:C$\backslash$:/Users/PersianNB/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Okarma, K. - 2014 - Quality Assessment of Images with Multiple Distortions using Combined Metrics.pdf:pdf},
issn = {2029-5731},
journal = {Elektronika ir Elektrotechnika},
keywords = {Image analysis,Multiple Distortions,Okarma2014,image quality},
mendeley-groups = {MD,2- MLIVE-cited},
mendeley-tags = {Multiple Distortions,Okarma2014},
month = {jun},
number = {6},
pages = {128--131},
title = {{Quality Assessment of Images with Multiple Distortions using Combined Metrics}},
url = {http://eejournal.ktu.lt/index.php/elt/article/view/7284},
volume = {20},
year = {2014}
}
@article{Muraleetharan2015,
abstract = {Parallel to the quantization of the complex plane, using the canonical coherent states of a right quaternionic Hilbert space, quaternion field of quaternionic quantum mechanics is quantized. Associated upper symbols, lower symbols and related quantities are analysed. Quaternionic version of the harmonic oscillator and Weyl-Heisenberg algebra are also obtained.},
author = {Muraleetharan, B. and Thirulogasanthar, K.},
doi = {10.1063/1.4928934},
issn = {00222488},
journal = {Journal of Mathematical Physics},
title = {{Coherent state quantization of quaternions}},
year = {2015}
}
@inproceedings{chen2013image,
author = {Chen, Qiwei and Xu, Yi and Li, Chuan and Liu, Ning and Yang, Xiaokang},
booktitle = {2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)},
organization = {IEEE},
pages = {1--6},
title = {{An image quality assessment metric based on quaternion wavelet transform}},
year = {2013}
}
@inproceedings{traore2014reduced,
author = {Traor{\'{e}}, Albekaye and Carr{\'{e}}, Philippe and Olivier, Christian},
booktitle = {2014 IEEE International Conference on Image Processing (ICIP)},
organization = {IEEE},
pages = {526--530},
title = {{Reduced-reference metric based on the quaternionic wavelet coefficients modeling by information criteria}},
year = {2014}
}
@article{Tang2017,
abstract = {As an extension of Discrete and Complex Wavelet Transform, Quaternion Wavelet Transform (QWT) has attracted extensive attention in the past few years, because it can provide better analytic representation for 2D images. The QWT of an image consists of four parts, i.e., one magnitude part and three phase parts. The magnitude is nearly shift-invariant, which characterizes features at any spatial location, and the three phases represent the structure of these features. This indicates that QWT is more powerful in representing image structures, and thus is suitable for image quality evaluation. In this paper, an efficient and effective Camera Image Quality Metric (CIQM) is proposed based on QWT, which is utilized to describe the intrinsic structures of an image. For an image, it is first decomposed by QWT with three scales. Then, for each scale, the magnitude and entropy of the subband coefficients, and natural scene statistics of the third phase are calculated. The magnitude is utilized to describe the generalized spectral behavior, and the entropy is used to encode the generalized information of distortions. Since the third phase of QWT is considered to be texture feature, the natural scene statistics of the third phase of QWT is used to measure structure degradations in the proposed method. All these features reflect the self-similarity and independency of image content, which can effectively reflect image distortions. Finally, random forest is utilized to build the quality model. Experiments conducted on three camera image databases and two multiply distorted image databases have proved that CIQM outperforms the relevant state-of-the-art models for both authentically distorted images and multiply distorted images.},
author = {Tang, Lijuan and Li, Leida and Sun, Kezheng and Xia, Zhifang and Gu, Ke and Qian, Jiansheng},
doi = {10.1016/J.JVCIR.2017.09.010},
issn = {1047-3203},
journal = {Journal of Visual Communication and Image Representation},
mendeley-groups = {2- MLIVE-cited},
month = {nov},
pages = {204--212},
publisher = {Academic Press},
title = {{An efficient and effective blind camera image quality metric via modeling quaternion wavelet coefficients}},
url = {https://www.sciencedirect.com/science/article/pii/S1047320317301840},
volume = {49},
year = {2017}
}

@inproceedings{ngan1986cosine,
author = {Ngan, King N and Leong, Kin S and Singh, Harcharan},
booktitle = {Visual Communications and Image Processing},
organization = {International Society for Optics and Photonics},
pages = {165--171},
title = {{Cosine transform coding incorporating human visual system model}},
volume = {707},
year = {1986}
}

@misc{Friston2010,
abstract = {A free-energy principle has been proposed recently that accounts for action, perception and learning. This Review looks at some key brain theories in the biological (for example, neural Darwinism) and physical (for example, information theory and optimal control theory) sciences from the free-energy perspective. Crucially, one key theme runs through each of these theories - optimization. Furthermore, if we look closely at what is optimized, the same quantity keeps emerging, namely value (expected reward, expected utility) or its complement, surprise (prediction error, expected cost). This is the quantity that is optimized under the free-energy principle, which suggests that several global brain theories might be unified within a free-energy framework.},
author = {Friston, Karl},
booktitle = {Nature Reviews Neuroscience},
doi = {10.1038/nrn2787},
issn = {1471003X},
title = {{The free-energy principle: A unified brain theory?}},
year = {2010}
}

@article{Gabarda2007,
abstract = {We describe an innovative methodology for determining the quality of digital images. The method is based on measuring the variance of the expected entropy of a given image upon a set of predefined directions. Entropy can be calculated on a local basis by using a spatial/spatial-frequency distribution as an approximation for a probability density function. The generalized R{\'{e}}nyi entropy and the normalized pseudo-Wigner distribution (PWD) have been selected for this purpose. As a consequence, a pixel-by-pixel entropy value can be calculated, and therefore entropy histograms can be generated as well. The variance of the expected entropy is measured as a function of the directionality, and it has been taken as an anisotropy indicator. For this purpose, directional selectivity can be attained by using an oriented 1-D PWD implementation. Our main purpose is to show how such an anisotropy measure can be used as a metric to assess both the fidelity and quality of images. Experimental results show that an index such as this presents some desirable features that resemble those from an ideal image quality function, constituting a suitable quality index for natural images. Namely, in-focus, noise-free natural images have shown a maximum of this metric in comparison with other degraded, blurred, or noisy versions. This result provides a way of identifying in-focus, noise-free images from other degraded versions, allowing an automatic and nonreference classification of images according to their relative quality. It is also shown that the new measure is well correlated with classical reference metrics such as the peak signal-to-noise ratio.},
author = {Gabarda, Salvador and Crist{\'{o}}bal, Gabriel},
doi = {10.1364/josaa.24.000b42},
issn = {1084-7529},
journal = {Journal of the Optical Society of America A},
title = {{Blind image quality assessment through anisotropy}},
year = {2007}
}

@article{Dabov2007,
abstract = {We propose a novel image denoising strategy based on an enhanced sparse representation in transform domain. The enhancement of the sparsity is achieved by grouping similar 2-D image fragments (e.g., blocks) into 3-D data arrays which we call "groups." Collaborative filtering is a special procedure developed to deal with these 3-D groups. We realize it using the three successive steps: 3-D transformation of a group, shrinkage of the transform spectrum, and inverse 3-D transformation. The result is a 3-D estimate that consists of the jointly filtered grouped image blocks. By attenuating the noise, the collaborative filtering reveals even the finest details shared by grouped blocks and, at the same time, it preserves the essential unique features of each individual block. The filtered blocks are then returned to their original positions. Because these blocks are overlapping, for each pixel, we obtain many different estimates which need to be combined. Aggregation is a particular averaging procedure which is exploited to take advantage of this redundancy. A significant improvement is obtained by a specially developed collaborative Wiener filtering. An algorithm based on this novel denoising strategy and its efficient implementation are presented in full detail; an extension to color-image denoising is also developed. The experimental results demonstrate that this computationally scalable algorithm achieves state-of-the-art denoising performance in terms of both peak signal-to-noise ratio and subjective visual quality.},
author = {Dabov, Kostadin and Foi, Alessandro and Katkovnik, Vladimir and Egiazarian, Karen},
doi = {10.1109/TIP.2007.901238},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {3-D transform shrinkage,Adaptive grouping,Block matching,Image denoising,Sparsity},
title = {{Image denoising by sparse 3-D transform-domain collaborative filtering}},
year = {2007}
}
@inproceedings{Zoran2009,
abstract = {Natural images are known to have scale invariant statistics. While some eariler studies have reported the kurtosis of marginal bandpass filter response distributions to be constant throughout scales, other studies have reported that the kurtosis values are lower for high frequency filters than for lower frequency ones. In this work we propose a resolution for this discrepancy and suggest that this change in kurtosis values is due to noise present in the image. We suggest that this effect is consistent with a clean, natural image corrupted by white noise. We propose a model for this effect, and use it to estimate noise standard deviation in corrupted natural images. In particular, our results suggest that classical benchmark images used in low-level vision are actually noisy and can be cleaned up. Our results on noise estimation on two sets of 50 and a 100 natural images are significantly better than the state-of-the-art.},
author = {Zoran, Daniel and Weiss, Yair},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2009.5459476},
isbn = {9781424444205},
title = {{Scale invariance and noise in natural images}},
year = {2009}
}
@inproceedings{Li2015relevant,
author = {Li, Chaofeng and Zhang, Yu and Wu, Xiaojun and Fang, Wei and Mao, Li},
booktitle = {2015 IEEE International Conference on Image Processing (ICIP)},
doi = {10.1109/ICIP.2015.7351735},
file = {::},
isbn = {978-1-4799-8339-1},
keywords = {LQAF,Li2015{\_}Chaofeng,Multiple Distortions},
mendeley-groups = {MD,2- MLIVE-cited},
mendeley-tags = {LQAF,Li2015{\_}Chaofeng,Multiple Distortions},
month = {sep},
pages = {4883--4886},
publisher = {IEEE},
title = {{Blind multiply distorted image quality assessment using relevant perceptual features}},
url = {http://ieeexplore.ieee.org/document/7351735/},
year = {2015}
}
@inproceedings{Ma2017forest,
author = {Ma, Mengzhu and Li, Chaofeng},
booktitle = {2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)},
doi = {10.1109/CISP-BMEI.2017.8301996},
file = {::},
isbn = {978-1-5386-1937-7},
keywords = {Ma2017,Multiple Distortions},
mendeley-groups = {MD,2- MLIVE-cited},
mendeley-tags = {Ma2017,Multiple Distortions},
month = {oct},
pages = {1--5},
publisher = {IEEE},
title = {{Blind multiply distorted image quality assessment using an ensemble random forest}},
url = {http://ieeexplore.ieee.org/document/8301996/},
year = {2017}
}
@article{Li2018access,
author = {Li, Chaofeng and Zhang, Yu and Wu, Xiaojun and Zheng, Yuhui},
doi = {10.1109/ACCESS.2018.2877714},
issn = {21693536},
journal = {IEEE Access},
keywords = {Blind image quality assessment,Local amplitude,Multi-scale images,Multiply distorted images,Phase congruency,Support vector regression},
title = {{A multi-scale learning local phase and amplitude blind image quality assessment for multiply distorted images}},
year = {2018}
}

@article{He2012srnss,
abstract = {Blind image quality assessment (BIQA) is an important yet difficult $\backslash$ntask in image processing related applications. Existing algorithms for universal $\backslash$nBIQA learn a mapping from features of an image to the corresponding subjective $\backslash$nquality or divide the image into different distortions before mapping. Although $\backslash$nthese algorithms are promising, they face the following problems: (1) they $\backslash$nrequire a large number of samples (pairs of distorted image and its subjective $\backslash$nquality) to train a robust mapping; (2) they are sensitive to different $\backslash$ndatasets; and (3) they have to be retrained when new training samples are $\backslash$navailable. In this paper, we introduce a simple yet effective algorithm based $\backslash$nupon the sparse representation of natural scene statistics (NSS) feature. It $\backslash$nconsists of three key steps: extracting NSS features in the wavelet domain, $\backslash$nrepresenting features via sparse coding, and weighting differential mean opinion $\backslash$nscores by the sparse coding coefficients to obtain the final visual quality $\backslash$nvalues. Thorough experiments on standard databases show that the proposed $\backslash$nalgorithm outperforms representative BIQA algorithms and some full-reference $\backslash$nmetrics.},
author = {He, Lihuo and Tao, Dacheng and Li, Xuelong and Gao, Xinbo},
doi = {10.1109/CVPR.2012.6247795},
isbn = {1063-6919 VO -},
issn = {10636919},
journal = {Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on},
keywords = {Feature-SVR,Image Quality,No Reference,SRNSS,feature extraction,image coding,natural scenes,spa},
mendeley-groups = {IQA-NR},
mendeley-tags = {Feature-SVR,Image Quality,No Reference,SRNSS},
pages = {1146--1153},
title = {{Sparse representation for blind image quality assessment}},
year = {2012}
}


@article{Tibshirani1996,
abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
author = {Tibshirani, Robert},
doi = {10.1111/j.2517-6161.1996.tb02080.x},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
title = {{Regression Shrinkage and Selection Via the Lasso}},
year = {1996}
}

@article{Legge1980,
abstract = {Contrast masking was studied psychophysically. A two-alternative forced-choice procedure was used to measure contrast thresholds for 2.0 cpd sine-wave gratings in the presence of masking sine-wave gratings. Thresholds were measured for 11 masker contrasts spanning three log units, and seven masker frequencies ranging +/- one octave from the signal frequency. Corresponding measurements were made for gratings with horizontal widths of 0.75 degrees (narrow fields) and 6.0 degrees (wide fields). For high contrast maskers at all frequencies, signal thresholds were related to masking contrast by power functions with exponents near 0.6. For a range of low masking contrasts, signal thresholds were reduced by the masker. For the wide fields, high contrast masking tuning functions peaked at the signal frequency, were slightly asymmetric, and had approximately invariant half-maximum frequencies that lie 3/4 octave below and 1 octave above the signal frequency. The corresponding low contrast tuning functions exhibited peak threshold reduction at the signal frequency, with half-minimum frequencies at roughly +/- 0.25 octaves. For the narrow fields, the masking tuning functions were much broader at both low and high masking contrasts. A masking model is presented that encompasses contrast detection, discrimination, and masking phenomena. Central constructs of the model include a linear spatial frequency filter, a nonlinear transducer, and a process of spatial pooling that acts at low contrasts only.},
author = {Legge, G. E. and Foley, J. M.},
doi = {10.1364/JOSA.70.001458},
issn = {00303941},
journal = {Journal of the Optical Society of America},
title = {{Contrast masking in human vision.}},
year = {1980}
}

@article{Martin2001,
abstract = {This paper presents a database containing `ground truth'$\backslash$nsegmentations produced by humans for images of a wide variety of natural$\backslash$nscenes. We define an error measure which quantifies the consistency$\backslash$nbetween segmentations of differing granularities and find that different$\backslash$nhuman segmentations of the same image are highly consistent. Use of this$\backslash$ndataset is demonstrated in two applications: (1) evaluating the$\backslash$nperformance of segmentation algorithms and (2) measuring probability$\backslash$ndistributions associated with Gestalt grouping factors as well as$\backslash$nstatistics of image region properties},
author = {Martin, David and Fowlkes, Charless and Tal, Doron and Malik, Jitendra},
doi = {10.1109/ICCV.2001.937655},
isbn = {0-7695-1143-0},
issn = {1063-6919},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {416--423},
pmid = {21914436},
title = {{A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics}},
volume = {2},
year = {2001}
}

@article{Zhang2014cdiivine,
abstract = {It is widely known that the wavelet coefficients of natural scenes possess certain statistical regularities which can be affected by the presence of distortions. The DIIVINE (Distortion Identification-based Image Verity and Integrity Evaluation) algorithm is a successful no-reference image quality assessment (NR IQA) algorithm, which estimates quality based on changes in these regularities. However, DIIVINE operates based on real-valued wavelet coefficients, whereas the visual appearance of an image can be strongly determined by both the magnitude and phase information. In this paper, we present a complex extension of the DIIVINE algorithm (called C-DIIVINE), which blindly assesses image quality based on the complex Gaussian scale mixture model corresponding to the complex version of the steerable pyramid wavelet transform. Specifically, we applied three commonly used distribution models to fit the statistics of the wavelet coefficients: (1) the complex generalized Gaussian distribution is used to model the wavelet coefficient magnitudes, (2) the generalized Gaussian distribution is used to model the coefficients' relative magnitudes, and (3) the wrapped Cauchy distribution is used to model the coefficients' relative phases. All these distributions have characteristic shapes that are consistent across different natural images but change significantly in the presence of distortions. We also employ the complex wavelet structural similarity index to measure degradation of the correlations across image scales, which serves as an important indicator of the subbands' energy distribution and the loss of alignment of local spectral components contributing to image structure. Experimental results show that these complex extensions allow C-DIIVINE to yield a substantial improvement in predictive performance as compared to its predecessor, and highly competitive performance relative to other recent no-reference algorithms. {\textcopyright} 2014 Elsevier B.V.},
author = {Zhang, Yi and Moorthy, Anush K. and Chandler, Damon M. and Bovik, Alan C.},
doi = {10.1016/j.image.2014.05.004},
issn = {09235965},
journal = {Signal Processing: Image Communication},
keywords = {Complex Gaussian scale mixture,Complex wavelet transform,Image quality assessment,Relative phase},
title = {{C-DIIVINE: No-reference image quality assessment based on local magnitude and phase statistics of natural scenes}},
year = {2014}
}

@article{Zhang2013desique,
abstract = {In this paper, we propose a new method for blind/no-reference image quality assessment based on the log-derivative statistics of natural scenes. The new method, called DErivative Statistics-based Image QUality Eval-uator (DESIQUE), extracts image quality-related statistical features at two image scales in both the spatial and frequency domains, upon which a two-stage framework is employed to evaluate image quality. In the spatial domain, normalized luminance values of an image are modeled in two ways: point-wise based statistics for sin-gle pixel values and pairwise-based log-derivative statistics for the relationship of pixel pairs. In the frequency domain, log-Gabor filters are used to extract the high frequency component of an image, which is also modeled by the log-derivative statistics. All of these statistics are characterized by a generalized Gaussian distribution model, the parameters of which form the underlying features of the proposed method. Experiment results show that DESIQUE not only leads to considerable performance improvements, but also maintains high computational efficiency.},
author = {Zhang, Yi and Chandler, Damon M.},
doi = {10.1117/12.2001342},
file = {::},
issn = {1017-9909},
journal = {Image Quality and System Performance},
keywords = {DESIQUE,Feature-SVR,Image Quality,No Reference,Zhang2013,algorithm for no-reference image,and image quality,based on log-derivative statistics,chandler,derivative statistics,generalized gaussian distribution,image quality assessment,laboratory of computational perception,log-gabor filer,m,of natural scenes,quality assessment,yi zhang and damon},
mendeley-groups = {IQA-NR},
mendeley-tags = {DESIQUE,Feature-SVR,Image Quality,No Reference,Zhang2013},
pages = {86530J},
title = {{An algorithm for no-reference image quality assessment based on log-derivative statistics of natural scenes}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2001342},
volume = {8653},
year = {2013}
}
@article{Ojala2002,
abstract = {Presents a theoretically very simple, yet efficient,$\backslash$nmultiresolution approach to gray-scale and rotation invariant texture$\backslash$nclassification based on local binary patterns and nonparametric$\backslash$ndiscrimination of sample and prototype distributions. The method is$\backslash$nbased on recognizing that certain local binary patterns, termed$\backslash$n"uniform," are fundamental properties of local image texture and their$\backslash$noccurrence histogram is proven to be a very powerful texture feature. We$\backslash$nderive a generalized gray-scale and rotation invariant operator$\backslash$npresentation that allows for detecting the "uniform" patterns for any$\backslash$nquantization of the angular space and for any spatial resolution and$\backslash$npresents a method for combining multiple operators for multiresolution$\backslash$nanalysis. The proposed approach is very robust in terms of gray-scale$\backslash$nvariations since the operator is, by definition, invariant against any$\backslash$nmonotonic transformation of the gray scale. Another advantage is$\backslash$ncomputational simplicity as the operator can be realized with a few$\backslash$noperations in a small neighborhood and a lookup table. Experimental$\backslash$nresults demonstrate that good discrimination can be achieved with the$\backslash$noccurrence statistics of simple rotation invariant local binary patterns$\backslash$n},
author = {Ojala, Timo and Pietik{\"{a}}inen, Matti and M{\"{a}}enp{\"{a}}{\"{a}}, Topi},
doi = {10.1109/TPAMI.2002.1017623},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Brodatz,Contrast,Distribution,Histogram,Nonparametric,Outex,Texture analysis},
number = {7},
pages = {971--987},
title = {{Multiresolution gray-scale and rotation invariant texture classification with local binary patterns}},
volume = {24},
year = {2002}
}

@inproceedings{Dalal2005,
abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
author = {Dalal, Navneet and Triggs, Bill},
booktitle = {Proceedings - 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2005},
doi = {10.1109/CVPR.2005.177},
isbn = {0769523722},
title = {{Histograms of oriented gradients for human detection}},
year = {2005}
}

@article{Hughes1996,
abstract = {A great deal of evidence suggests that early in processing, retinal images are filtered by parallel, spatial frequency selective channels. We attempt to incorporate this view of early vision with the principle of global precedence, which holds that Gestalt-like processes sensitive to global image configurations tend to dominate local feature processing in human pattern perception. Global precedence is inferred from the pattern of reaction times observed when visual patterns contain multiple cues at different levels of spatial scale. Specifically, it is frequently observed that global processing times are largely unaffected by conflicting local cues, but local processing times are substantially lengthened by conflicting global cues. The asymmetry of these effects suggests the dominant role of global configurations. Since global spatial information is effectively represented by low spatial frequencies, global precedence potentially implies a low frequency dominance. The thesis is that low spatial frequencies tend to be available before information carried by higher frequency bands, producing a coarse-to-fine temporal order in visual spatial perception. It is suggested that a variety of factors contribute to the ''prior entry'' of low frequency information, including the high contrast gain of the magnocellular pathway the amplitude spectra typical of natural images, and inhibitory interactions between the parallel frequency-tuned channels. Evidence suggesting a close relationship between global precedence and spatial frequency channels is provided by observations that the essential features of the global precedence effect are obtained using patterns consisting of low and high frequency sinusoids. The hypothesis that these asymmetric interference effects are due to interactions between parallel spatial channels is supported by an analysis of reaction times (RTs), which shows that RTs to redundant low and high frequency cues produce less facilitation than predictions that assume the channels are independent. In view of previous work showing that global precedence depends upon the low frequency content of the stimuli, we suggest that low spatial frequencies represent the sine qua non for the dominance of configurational cues in human pattern perception, and that this configurational dominance reflects the microgenesis of visual pattern perception. This general view of the temporal dynamics of visual pattern recognition is discussed, is considered from an evolutionary perspective, and is related to certain statistical regularities in natural scenes. Potential adaptive advantages of an interactive parallel architecture that confers an initial processing advantage to low resolution information are explored.},
author = {Hughes, Howard C. and Nozawa, George and Kitterle, Frederick},
doi = {10.1162/jocn.1996.8.3.197},
issn = {0898929X},
journal = {Journal of Cognitive Neuroscience},
title = {{Global precedence, spatial frequency channels, and the statistics of natural images}},
year = {1996}
}

@article{Heydari2019,
abstract = {{\textcopyright} 2019 Elsevier B.V. The aim of blind image quality assessment (BIQA) methods is to evaluate the perceptual quality of a distorted image without any prior information regarding its reference image. Although some impressive image quality metrics have been proposed, due to the complexity of the human visual system and the lack of a reference image, designing an image quality metric which accurately predicts human judgments is still a challenging issue. In this paper, a low complexity wavelet-based image quality assessment is proposed. Firstly, the interaction of fine and coarse details of the image, which is extracted by Haar wavelet, is analyzed. In the proposed approach, the joint statistics of two normalized high frequency subbands which indicate coarse and fine structures is utilized for extracting features. Actually, analyzing the relation between image details of different granularities is the main idea of the proposed method. After feature extraction phase, support vector regression (SVR) is adopted in order to provide a quality score. Experimental results show the effectiveness of the proposed low complexity approach.},
author = {Heydari, M. and Cheraaqee, P. and Mansouri, A. and Mahmoudi-Aznaveh, A.},
doi = {10.1016/j.image.2018.12.016},
issn = {09235965},
journal = {Signal Processing: Image Communication},
keywords = {BIQA,Human visual system,Image quality assessment,NR-IQA,Wavelet},
title = {{A low complexity wavelet-based blind image quality evaluator}},
volume = {74},
year = {2019}
}

@article{xue2014blind,
author = {Xue, Wufeng and Mou, Xuanqin and Zhang, Lei and Bovik, Alan C and Feng, Xiangchu},
file = {::},
journal = {IEEE Transactions on Image Processing},
keywords = {Feature-SVR,GMLOG,No Reference},
mendeley-groups = {IQA-NR},
mendeley-tags = {Feature-SVR,GMLOG,No Reference},
number = {11},
pages = {4850--4862},
publisher = {IEEE},
title = {{Blind image quality assessment using joint statistics of gradient magnitude and Laplacian features}},
url = {http://ieeexplore.ieee.org/abstract/document/6894197/},
volume = {23},
year = {2014}
}

@article{Gu2017micromacro,
abstract = {{\textcopyright} 2017 IEEE. A fast reliable computational quality predictor is eagerly desired in practical image/video applications, such as serving for the quality monitoring of real-time coding and transcoding. In this paper, we propose a new perceptual image quality assessment (IQA) metric based on the human visual system (HVS). The proposed IQA model performs efficiently with convolution operations at multiscales, gradient magnitude, and color information similarity, and a perceptual-based pooling. Extensive experiments are conducted using four popular large-size image databases and two multiply distorted image databases, and results validate the superiority of our approach over modern IQA measures in efficiency and efficacy. Our metric is built on the theoretical support of the HVS with lately designed IQA methods as special cases.},
author = {Gu, Ke and Li, Leida and Lu, Hong and Min, Xiongkuo and Lin, Weisi},
doi = {10.1109/TIE.2017.2652339},
isbn = {0278-0046},
issn = {02780046},
journal = {IEEE Transactions on Industrial Electronics},
keywords = {Color information,Gradient operator,Perceptual image quality assessment (IQA),Pooling,Structure},
mendeley-groups = {2- MLIVE-cited},
title = {{A fast reliable image quality predictor by fusing micro- and macro-structures}},
year = {2017}
}
@article{Miao2019,
author = {Miao, Xikui and Chu, Hairong and Liu, Hui and Yang, Yao and Li, Xiaolong},
doi = {10.1016/j.image.2019.08.013},
issn = {09235965},
journal = {Signal Processing: Image Communication},
title = {{Quality assessment of images with multiple distortions based on phase congruency and gradient magnitude}},
year = {2019}
}

@article{Larsson2006,
abstract = {Second-order textures-patterns that cannot be detected by mechanisms sensitive only to luminance changes-are ubiquitous in visual scenes, but the neuronal mechanisms mediating perception of such stimuli are not well understood. We used an adaptation protocol to measure neural activity in the human brain selective for the orientation of second-order textures. Functional MRI (fMRI) responses were measured in three subjects to presentations of first- and second-order probe gratings after adapting to a high-contrast first- or second-order grating that was either parallel or orthogonal to the probe gratings. First-order (LM) stimuli were generated by modulating the stimulus luminance. Second-order stimuli were generated by modulating the contrast (CM) or orientation (OM) of a first-order carrier. We used four combinations of adapter and probe stimuli: LM:LM, CM:CM, OM:OM, and LM:OM. The fourth condition tested for cross-modal adaptation with first-order adapter and second-order probe stimuli. Attention was diverted from the stimulus by a demanding task at fixation. Both first- and second-order stimuli elicited orientation-selective adaptation in multiple cortical visual areas, including V1, V2, V3, V3A/B, a newly identified visual area anterior to dorsal V3 that we have termed LO1, hV4, and VO1. For first-order stimuli (condition LM:LM), the adaptation was no larger in extrastriate areas than in V1, implying that the orientation-selective first-order (luminance) adaptation originated in V1. For second-order stimuli (conditions CM:CM and OM:OM), the magnitude of adaptation, relative to the absolute response magnitude, was significantly larger in VO1 (and for condition CM:CM, also in V3A/B and LO1) than in V1, suggesting that second-order stimulus orientation was extracted by additional processing after V1. There was little difference in the amplitude of adaptation between the second-order conditions. No consistent effect of adaptation was found in the cross-modal condition LM:OM, in agreement with psychophysical evidence for weak interactions between first- and second-order stimuli and computational models of separate mechanisms for first- and second-order visual processing.},
author = {Larsson, Jonas and Landy, Michael S. and Heeger, David J.},
doi = {10.1152/jn.00668.2005},
issn = {00223077},
journal = {Journal of Neurophysiology},
title = {{Orientation-selective adaptation to first- and second-order patterns in human visual cortex}},
year = {2006}
}
@article{Griffin2007,
abstract = {Characterization of second order local image structure by a 6D vector (or jet) of Gaussian derivative measurements is considered. We consider the affect on jets of a group of transformations - affine intensity-scaling, image rotation and reflection, and their compositions - that preserve intrinsic image structure. We show how this group stratifies the jet space into a system of orbits. Considering individual orbits as points, a 3D orbifold is defined. We propose a norm on jet space which we use to induce a metric on the orbifold. The metric tensor shows that the orbifold is intrinsically curved. To allow visualization of the orbifold and numerical computation with it, we present a mildly-distorting but volume-preserving embedding of it into euclidean 3-space. We call the resulting shape, which is like a flattened lemon, the second order local-image-structure solid. As an example use of the solid, we compute the distribution of local structures in noise and natural images. For noise images, analytical results are possible and they agree with the empirical results. For natural images, an excess of locally 1D structure is found. {\textcopyright} 2007 IEEE.},
author = {Griffin, Lewis D.},
doi = {10.1109/TPAMI.2007.1066},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Feature analysis,Image derivatives,Natural images,Noise,Scale space},
title = {{The second order local-image-structure solid}},
year = {2007}
}

@article{VanDeWeijer2006,
abstract = {The aim of salient feature detection is to find distinctive local events in images. Salient features are generally determined from the local differential structure of images. They focus on the shape-saliency of the local neighborhood. The majority of these detectors are luminance-based, which has the disadvantage that the distinctiveness of the local color information is completely ignored in determining salient image features. To fully exploit the possibilities of salient point detection in color images, color distinctiveness should be taken into account in addition to shape distinctiveness. In this paper, color distinctiveness is explicitly incorporated into the design of saliency detection. The algorithm, called color saliency boosting, is based on an analysis of the statistics of color image derivatives. Color saliency boosting is designed as a generic method easily adaptable to existing feature detectors. Results show that substantial improvements in information content are acquired by targeting color salient features.},
author = {{Van De Weijer}, Joost and Gevers, Theo and Bagdanov, Andrew D.},
doi = {10.1109/TPAMI.2006.3},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Color imaging,Feature detection,Image saliency,Image statistics},
title = {{Boosting color saliency in image feature detection}},
year = {2006}
}
@article{Ho1998,
abstract = {Much of previous attention on decision trees focuses on the splitting criteria and optimization of tree sizes. The dilemma between overfitting and achieving maximum accuracy is seldom resolved. A method to construct a decision tree based classifier is proposed that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces. The subspace method is compared to single-tree classifiers and other forest construction methods by experiments on publicly available datasets, where the method's superiority is demonstrated. We also discuss independence between trees in a forest and relate that to the combined classification accuracy. {\textcopyright} 1998 IEEE.},
author = {Ho, Tin Kam},
doi = {10.1109/34.709601},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Bootstrapping,Classifier combination,Decision combination,Decision forest,Decision tree,Multiple-classifier system,Pattern recognition,Stochastic discrimination},
title = {{The random subspace method for constructing decision forests}},
year = {1998}
}

@misc{Pelli2008,
abstract = {It is now emerging that vision is usually limited by object spacing rather than size. The visual system recognizes an object by detecting and then combining its features. 'Crowding' occurs when objects are too close together and features from several objects are combined into a jumbled percept. Here, we review the explosion of studies on crowding--in grating discrimination, letter and face recognition, visual search, selective attention, and reading--and find a universal principle, the Bouma law. The critical spacing required to prevent crowding is equal for all objects, although the effect is weaker between dissimilar objects. Furthermore, critical spacing at the cortex is independent of object position, and critical spacing at the visual field is proportional to object distance from fixation. The region where object spacing exceeds critical spacing is the 'uncrowded window'. Observers cannot recognize objects outside of this window and its size limits the speed of reading and search.},
author = {Pelli, Denis G. and Tillman, Katharine A.},
booktitle = {Nature Neuroscience},
doi = {10.1038/nn.2187},
issn = {10976256},
title = {{The uncrowded window of object recognition}},
year = {2008}
}
@inproceedings{Zheng2009,
abstract = {Medical image segmentation is the basis of medical image three-dimension reconstruction. The accuracy of image segmentation directly affects the results of image 3D reconstruction. Medical image is a kind of grayscale image. In order to adequately utilize gray information and spatial information of image, the traditional 2D gray histogram is improved and forms the 2D D-value attribute gray histogram. Computation method of average gray and 2D entropy is improved. Use spatial information as a substitute for gray probability to compute entropy. Computation of entropy is based on D-value attribute gray histogram and created spatial different attribute information entropy (SDAIVE). In experiment, a series of head CT images are segmented. Experimental results show that improved threshold method can better segment noise image. This method has strong anti-noise capability and clear segmentation results.},
author = {Zheng, Liping and Jiang, Hua and Pan, Quanke and Li, Guangyao},
booktitle = {ICCIT 2009 - 4th International Conference on Computer Sciences and Convergence Information Technology},
doi = {10.1109/ICCIT.2009.66},
isbn = {9780769538969},
keywords = {2D histogram,Entropy,Gray information,Gray probability,Image segmentation},
title = {{Medical image segmentation based on an improved 2D entropy}},
year = {2009}
}

@article{Wu2015,
abstract = {{\textcopyright} 1992-2012 IEEE. The human visual system is highly adaptive to extract structure information for scene perception, and structure character is widely used in perception-oriented image processing works. However, the existing structure descriptors mainly describe the luminance contrast of a local region, but cannot effectively represent the spatial correlation of structure. In this paper, we introduce a novel structure descriptor according to the orientation selectivity mechanism in the primary visual cortex. Research on cognitive neuroscience indicate that the arrangement of excitatory and inhibitory cortex cells arise orientation selectivity in a local receptive field, within which the primary visual cortex performs visual information extraction for scene understanding. Inspired by the orientation selectivity mechanism, we compute the correlations among pixels in a local region based on the similarities of their preferred orientation. By imitating the arrangement of the excitatory/inhibitory cells, the correlations between a central pixel and its local neighbors are binarized, and the spatial correlation is represented with a set of binary values, which is named the orientation selectivity-based pattern. Then, taking both the gradient magnitude and the orientation selectivity-based pattern into account, a rotation invariant structure descriptor is introduced. The proposed structure descriptor is applied in texture classification and reduced reference image quality assessment, as two different application domains to verify its generality and robustness. Experimental results demonstrate that the orientation selectivity-based structure descriptor is robust to disturbance, and can effectively represent the structure degradation caused by different types of distortion.},
author = {Wu, Jinjian and Lin, Weisi and Shi, Guangming and Zhang, Yazhong and Dong, Weisheng and Chen, Zhibo},
doi = {10.1109/TIP.2015.2460467},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Excitatory/Inhibitory Interaction,Orientation Selectivity,Texture Classification},
title = {{Visual Orientation Selectivity Based Structure Description}},
year = {2015}
}

@inproceedings{cheraaqee2019incorporating,
  title={Incorporating Gradient Direction for Assessing Multiple Distortions},
  author={Cheraaqee, Pooryaa and Mansouri, Azadeh and Mahmoudi-Aznaveh, Ahmad},
  booktitle={2019 4th International Conference on Pattern Recognition and Image Analysis (IPRIA)},
  pages={109--113},
  year={2019},
  organization={IEEE}
}

@article{smola2004tutorial,
  title={A tutorial on support vector regression},
  author={Smola, Alex J and Sch{\"o}lkopf, Bernhard},
  journal={Statistics and computing},
  volume={14},
  number={3},
  pages={199--222},
  year={2004},
  publisher={Springer}
}

@article{chang2011libsvm,
  title={LIBSVM: A library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  volume={2},
  number={3},
  pages={27},
  year={2011},
  publisher={Acm}
}